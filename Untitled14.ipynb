{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yhola/AI-Powered-Halliday-Model-of-Systemic-Functional-Linguistics-/blob/main/Untitled14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJgKvY9V7S71",
        "outputId": "2ef16acc-ff7d-463f-cf4a-602c6be92c83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement re (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for re\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.6)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries for the project\n",
        "!pip install PyPDF2\n",
        "!pip install nltk\n",
        "!pip install scikit-learn\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install re\n",
        "!pip install spacy\n",
        "!pip install textblob\n",
        "\n",
        "# Additional setup\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import spacy\n",
        "# Download SpaCy model for NLP tasks\n",
        "!python -m spacy download en_core_web_sm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAqipEI2BFrb",
        "outputId": "157ea45b-52eb-4d49-b8d1-ccbcc7940065"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of words in Dataset 1 (Dissertations): 173859\n",
            "Total number of words in Dataset 2 (Peer-Reviewed Articles): 71779\n"
          ]
        }
      ],
      "source": [
        "import fitz  # PyMuPDF\n",
        "import pytesseract\n",
        "from pdf2image import convert_from_path\n",
        "import os\n",
        "\n",
        "# Function to count words in a PDF using PyMuPDF (Fitz)\n",
        "def count_words_in_pdf_pymupdf(file_path):\n",
        "    total_word_count = 0\n",
        "\n",
        "    # Open the PDF with PyMuPDF\n",
        "    with fitz.open(file_path) as pdf:\n",
        "        for page_num in range(len(pdf)):\n",
        "            text = pdf.load_page(page_num).get_text()\n",
        "            total_word_count += len(text.split())\n",
        "\n",
        "    return total_word_count\n",
        "\n",
        "# Function to count words in scanned PDFs using OCR\n",
        "def count_words_in_scanned_pdf(file_path):\n",
        "    total_word_count = 0\n",
        "\n",
        "    # Convert PDF pages to images\n",
        "    images = convert_from_path(file_path)\n",
        "\n",
        "    for image in images:\n",
        "        # Use Tesseract to extract text from each image\n",
        "        text = pytesseract.image_to_string(image)\n",
        "        total_word_count += len(text.split())\n",
        "\n",
        "    return total_word_count\n",
        "\n",
        "# Function to decide whether to use PyMuPDF or OCR for word counting\n",
        "def count_words_in_pdf(file_path):\n",
        "    try:\n",
        "        # Try extracting text using PyMuPDF first\n",
        "        word_count = count_words_in_pdf_pymupdf(file_path)\n",
        "\n",
        "        # If the word count is too low, assume it's a scanned PDF and use OCR\n",
        "        if word_count < 100:  # Threshold to determine if OCR is needed\n",
        "            word_count = count_words_in_scanned_pdf(file_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path} with PyMuPDF, using OCR instead: {e}\")\n",
        "        word_count = count_words_in_scanned_pdf(file_path)\n",
        "\n",
        "    return word_count\n",
        "\n",
        "# Define paths for the datasets manually mounted in Google Colab\n",
        "dataset_1_path = '/content/drive/MyDrive/dataset_1.pdf'  # Path to Dataset 1 (Dissertations)\n",
        "dataset_2_path = '/content/drive/MyDrive/dataset_2.pdf'  # Path to Dataset 2 (Peer-Reviewed Articles)\n",
        "\n",
        "# Count the total number of words in Dataset 1 and Dataset 2\n",
        "dataset_1_word_count = count_words_in_pdf(dataset_1_path)\n",
        "dataset_2_word_count = count_words_in_pdf(dataset_2_path)\n",
        "\n",
        "print(f\"Total number of words in Dataset 1 (Dissertations): {dataset_1_word_count}\")\n",
        "print(f\"Total number of words in Dataset 2 (Peer-Reviewed Articles): {dataset_2_word_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJmVFq_7Bpsx",
        "outputId": "7136537a-80b8-468f-8df7-c9368c05ef05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of words in Dataset 1 (Dissertations): 173859\n",
            "Total number of words in Dataset 2 (Peer-Reviewed Articles): 71779\n",
            "Total in-text citations in Dataset 1 (Dissertations): 1397\n",
            "In-text citations per 200 words in Dataset 1 (Dissertations): 1.6070493905981285\n",
            "Total in-text citations in Dataset 2 (Peer-Reviewed Articles): 404\n",
            "In-text citations per 200 words in Dataset 2 (Peer-Reviewed Articles): 1.1256774265453684\n"
          ]
        }
      ],
      "source": [
        "import fitz  # PyMuPDF\n",
        "import pytesseract\n",
        "from pdf2image import convert_from_path\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Function to count words in a PDF using PyMuPDF (Fitz)\n",
        "def count_words_in_pdf_pymupdf(file_path):\n",
        "    total_word_count = 0\n",
        "\n",
        "    # Open the PDF with PyMuPDF\n",
        "    with fitz.open(file_path) as pdf:\n",
        "        for page_num in range(len(pdf)):\n",
        "            text = pdf.load_page(page_num).get_text()\n",
        "            total_word_count += len(text.split())\n",
        "\n",
        "    return total_word_count\n",
        "\n",
        "# Function to count words in scanned PDFs using OCR\n",
        "def count_words_in_scanned_pdf(file_path):\n",
        "    total_word_count = 0\n",
        "\n",
        "    # Convert PDF pages to images\n",
        "    images = convert_from_path(file_path)\n",
        "\n",
        "    for image in images:\n",
        "        # Use Tesseract to extract text from each image\n",
        "        text = pytesseract.image_to_string(image)\n",
        "        total_word_count += len(text.split())\n",
        "\n",
        "    return total_word_count\n",
        "\n",
        "# Function to decide whether to use PyMuPDF or OCR for word counting\n",
        "def count_words_in_pdf(file_path):\n",
        "    try:\n",
        "        # Try extracting text using PyMuPDF first\n",
        "        word_count = count_words_in_pdf_pymupdf(file_path)\n",
        "\n",
        "        # If the word count is too low, assume it's a scanned PDF and use OCR\n",
        "        if word_count < 100:  # Threshold to determine if OCR is needed\n",
        "            word_count = count_words_in_scanned_pdf(file_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path} with PyMuPDF, using OCR instead: {e}\")\n",
        "        word_count = count_words_in_scanned_pdf(file_path)\n",
        "\n",
        "    return word_count\n",
        "\n",
        "# Function to count in-text citations in a PDF\n",
        "def count_in_text_citations(file_path):\n",
        "    total_citations = 0\n",
        "    citations_per_200_words = 0\n",
        "\n",
        "    # Open the PDF with PyMuPDF\n",
        "    with fitz.open(file_path) as pdf:\n",
        "        text = \"\"\n",
        "        for page_num in range(len(pdf)):\n",
        "            text += pdf.load_page(page_num).get_text() + \" \"\n",
        "\n",
        "        # Improved regex patterns to match various in-text citation formats\n",
        "        citation_patterns = [\n",
        "            r\"\\b([A-Z][a-zA-Z'`-]+, \\d{4})\\b\",                        # Author, Year\n",
        "            r\"\\b([A-Z][a-zA-Z'`-]+ et al\\., \\d{4})\\b\",                 # Author et al., Year\n",
        "            r\"\\b([A-Z][a-zA-Z'`-]+ & [A-Z][a-zA-Z'`-]+, \\d{4})\\b\",     # Author & Author, Year\n",
        "            r\"\\(\\s*[A-Z][a-zA-Z'`-]+, \\d{4}\\s*\\)\",                     # (Author, Year)\n",
        "            r\"\\(\\s*[A-Z][a-zA-Z'`-]+ et al\\., \\d{4}\\s*\\)\",             # (Author et al., Year)\n",
        "            r\"\\(\\s*[A-Z][a-zA-Z'`-]+ & [A-Z][a-zA-Z'`-]+, \\d{4}\\s*\\)\"  # (Author & Author, Year)\n",
        "        ]\n",
        "\n",
        "        citations = []\n",
        "        for pattern in citation_patterns:\n",
        "            citations.extend(re.findall(pattern, text))\n",
        "\n",
        "        total_citations = len(citations)\n",
        "\n",
        "        # Calculate citations per 200 words\n",
        "        word_count = len(text.split())\n",
        "        if word_count > 0:\n",
        "            citations_per_200_words = (total_citations / word_count) * 200\n",
        "\n",
        "    return total_citations, citations_per_200_words\n",
        "\n",
        "# Define paths for the datasets manually mounted in Google Colab\n",
        "dataset_1_path = '/content/drive/MyDrive/dataset_1.pdf'  # Path to Dataset 1 (Dissertations)\n",
        "dataset_2_path = '/content/drive/MyDrive/dataset_2.pdf'  # Path to Dataset 2 (Peer-Reviewed Articles)\n",
        "\n",
        "# Count the total number of words in Dataset 1 and Dataset 2\n",
        "dataset_1_word_count = count_words_in_pdf(dataset_1_path)\n",
        "dataset_2_word_count = count_words_in_pdf(dataset_2_path)\n",
        "\n",
        "print(f\"Total number of words in Dataset 1 (Dissertations): {dataset_1_word_count}\")\n",
        "print(f\"Total number of words in Dataset 2 (Peer-Reviewed Articles): {dataset_2_word_count}\")\n",
        "\n",
        "# Count in-text citations for Dataset 1 and Dataset 2\n",
        "dataset_1_citations, dataset_1_citations_per_200_words = count_in_text_citations(dataset_1_path)\n",
        "dataset_2_citations, dataset_2_citations_per_200_words = count_in_text_citations(dataset_2_path)\n",
        "\n",
        "print(f\"Total in-text citations in Dataset 1 (Dissertations): {dataset_1_citations}\")\n",
        "print(f\"In-text citations per 200 words in Dataset 1 (Dissertations): {dataset_1_citations_per_200_words}\")\n",
        "print(f\"Total in-text citations in Dataset 2 (Peer-Reviewed Articles): {dataset_2_citations}\")\n",
        "print(f\"In-text citations per 200 words in Dataset 2 (Peer-Reviewed Articles): {dataset_2_citations_per_200_words}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eilm-8ZKCFvh",
        "outputId": "83debf8e-38d1-44d7-dcea-43b3a7718ca2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of words in Dataset 1 (Dissertations): 173859\n",
            "Total number of words in Dataset 2 (Peer-Reviewed Articles): 71779\n",
            "Total in-text citations in Dataset 1 (Dissertations): 1392\n",
            "In-text citations per 200 words in Dataset 1 (Dissertations): 1.5840412396943437\n",
            "Total in-text citations in Dataset 2 (Peer-Reviewed Articles): 386\n",
            "In-text citations per 200 words in Dataset 2 (Peer-Reviewed Articles): 1.0201250049552704\n"
          ]
        }
      ],
      "source": [
        "import fitz  # PyMuPDF\n",
        "import pytesseract\n",
        "from pdf2image import convert_from_path\n",
        "import os\n",
        "import re\n",
        "import PyPDF2\n",
        "\n",
        "# Function to count words in a PDF using PyMuPDF (Fitz)\n",
        "def count_words_in_pdf_pymupdf(file_path):\n",
        "    total_word_count = 0\n",
        "\n",
        "    # Open the PDF with PyMuPDF\n",
        "    with fitz.open(file_path) as pdf:\n",
        "        for page_num in range(len(pdf)):\n",
        "            text = pdf.load_page(page_num).get_text()\n",
        "            if text:\n",
        "                total_word_count += len(text.split())\n",
        "\n",
        "    return total_word_count\n",
        "\n",
        "# Function to count words in a scanned PDF using OCR\n",
        "def count_words_in_scanned_pdf(file_path):\n",
        "    total_word_count = 0\n",
        "\n",
        "    # Convert PDF pages to images\n",
        "    images = convert_from_path(file_path)\n",
        "\n",
        "    for image in images:\n",
        "        # Use Tesseract to extract text from each image\n",
        "        text = pytesseract.image_to_string(image)\n",
        "        if text:\n",
        "            total_word_count += len(text.split())\n",
        "\n",
        "    return total_word_count\n",
        "\n",
        "# Function to decide whether to use PyMuPDF or OCR for word counting\n",
        "def count_words_in_pdf(file_path):\n",
        "    try:\n",
        "        # Try extracting text using PyMuPDF first\n",
        "        word_count = count_words_in_pdf_pymupdf(file_path)\n",
        "\n",
        "        # If the word count is too low, assume it's a scanned PDF and use OCR\n",
        "        if word_count < 100:  # Threshold to determine if OCR is needed\n",
        "            word_count = count_words_in_scanned_pdf(file_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path} with PyMuPDF, using OCR instead: {e}\")\n",
        "        word_count = count_words_in_scanned_pdf(file_path)\n",
        "\n",
        "    return word_count\n",
        "\n",
        "# Function to count in-text citations in a PDF using PyPDF2\n",
        "def count_in_text_citations_pypdf2(file_path):\n",
        "    total_citations = 0\n",
        "    citations_per_200_words = 0\n",
        "\n",
        "    # Open the PDF with PyPDF2\n",
        "    with open(file_path, 'rb') as pdf_file:\n",
        "        reader = PyPDF2.PdfReader(pdf_file)\n",
        "        text = \"\"\n",
        "        for page_num in range(len(reader.pages)):\n",
        "            page = reader.pages[page_num]\n",
        "            text += page.extract_text() + \" \"\n",
        "\n",
        "        # Improved regex patterns to match various in-text citation formats\n",
        "        citation_patterns = [\n",
        "            r\"\\b([A-Z][a-zA-Z'`-]+, \\d{4})\\b\",                        # Author, Year\n",
        "            r\"\\b([A-Z][a-zA-Z'`-]+ et al\\., \\d{4})\\b\",                 # Author et al., Year\n",
        "            r\"\\b([A-Z][a-zA-Z'`-]+ & [A-Z][a-zA-Z'`-]+, \\d{4})\\b\",     # Author & Author, Year\n",
        "            r\"\\(\\s*[A-Z][a-zA-Z'`-]+, \\d{4}\\s*\\)\",                     # (Author, Year)\n",
        "            r\"\\(\\s*[A-Z][a-zA-Z'`-]+ et al\\., \\d{4}\\s*\\)\",             # (Author et al., Year)\n",
        "            r\"\\(\\s*[A-Z][a-zA-Z'`-]+ & [A-Z][a-zA-Z'`-]+, \\d{4}\\s*\\)\"  # (Author & Author, Year)\n",
        "        ]\n",
        "\n",
        "        citations = []\n",
        "        for pattern in citation_patterns:\n",
        "            citations.extend(re.findall(pattern, text))\n",
        "\n",
        "        total_citations = len(citations)\n",
        "\n",
        "        # Calculate citations per 200 words\n",
        "        word_count = len(text.split())\n",
        "        if word_count > 0:\n",
        "            citations_per_200_words = (total_citations / word_count) * 200\n",
        "\n",
        "    return total_citations, citations_per_200_words\n",
        "\n",
        "# Define paths for the datasets manually mounted in Google Colab\n",
        "dataset_1_path = '/content/drive/MyDrive/dataset_1.pdf'  # Path to Dataset 1 (Dissertations)\n",
        "dataset_2_path = '/content/drive/MyDrive/dataset_2.pdf'  # Path to Dataset 2 (Peer-Reviewed Articles)\n",
        "\n",
        "# Count the total number of words in Dataset 1 and Dataset 2\n",
        "dataset_1_word_count = count_words_in_pdf(dataset_1_path)\n",
        "dataset_2_word_count = count_words_in_pdf(dataset_2_path)\n",
        "\n",
        "print(f\"Total number of words in Dataset 1 (Dissertations): {dataset_1_word_count}\")\n",
        "print(f\"Total number of words in Dataset 2 (Peer-Reviewed Articles): {dataset_2_word_count}\")\n",
        "\n",
        "# Count in-text citations for Dataset 1 and Dataset 2\n",
        "dataset_1_citations, dataset_1_citations_per_200_words = count_in_text_citations_pypdf2(dataset_1_path)\n",
        "dataset_2_citations, dataset_2_citations_per_200_words = count_in_text_citations_pypdf2(dataset_2_path)\n",
        "\n",
        "print(f\"Total in-text citations in Dataset 1 (Dissertations): {dataset_1_citations}\")\n",
        "print(f\"In-text citations per 200 words in Dataset 1 (Dissertations): {dataset_1_citations_per_200_words}\")\n",
        "print(f\"Total in-text citations in Dataset 2 (Peer-Reviewed Articles): {dataset_2_citations}\")\n",
        "print(f\"In-text citations per 200 words in Dataset 2 (Peer-Reviewed Articles): {dataset_2_citations_per_200_words}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4mBVEjkDXFJ",
        "outputId": "6464d17f-e5ad-4a1e-b4e4-b46e8ef19439"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.4.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Downloading pdfminer.six-20240706-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pdfminer.six\n",
            "Successfully installed pdfminer.six-20240706\n",
            "Total number of words in Dataset 1 (Dissertations): 173859\n",
            "Total number of words in Dataset 2 (Peer-Reviewed Articles): 71779\n",
            "Total in-text citations in Dataset 1 (Dissertations): 1088\n",
            "In-text citations per 200 words in Dataset 1 (Dissertations): 1.2514593649533865\n",
            "Total in-text citations in Dataset 2 (Peer-Reviewed Articles): 324\n",
            "In-text citations per 200 words in Dataset 2 (Peer-Reviewed Articles): 0.8696935940624623\n"
          ]
        }
      ],
      "source": [
        "import fitz  # PyMuPDF\n",
        "import pytesseract\n",
        "from pdf2image import convert_from_path\n",
        "import os\n",
        "import re\n",
        "import PyPDF2\n",
        "\n",
        "# Install pdfminer package\n",
        "!pip install pdfminer.six\n",
        "from pdfminer.high_level import extract_text\n",
        "\n",
        "# Function to count words in a PDF using PyMuPDF (Fitz)\n",
        "def count_words_in_pdf_pymupdf(file_path):\n",
        "    total_word_count = 0\n",
        "\n",
        "    # Open the PDF with PyMuPDF\n",
        "    with fitz.open(file_path) as pdf:\n",
        "        for page_num in range(len(pdf)):\n",
        "            text = pdf.load_page(page_num).get_text()\n",
        "            if text:\n",
        "                total_word_count += len(text.split())\n",
        "\n",
        "    return total_word_count\n",
        "\n",
        "# Function to count words in a scanned PDF using OCR\n",
        "def count_words_in_scanned_pdf(file_path):\n",
        "    total_word_count = 0\n",
        "\n",
        "    # Convert PDF pages to images\n",
        "    images = convert_from_path(file_path)\n",
        "\n",
        "    for image in images:\n",
        "        # Use Tesseract to extract text from each image\n",
        "        text = pytesseract.image_to_string(image)\n",
        "        if text:\n",
        "            total_word_count += len(text.split())\n",
        "\n",
        "    return total_word_count\n",
        "\n",
        "# Function to decide whether to use PyMuPDF or OCR for word counting\n",
        "def count_words_in_pdf(file_path):\n",
        "    try:\n",
        "        # Try extracting text using PyMuPDF first\n",
        "        word_count = count_words_in_pdf_pymupdf(file_path)\n",
        "\n",
        "        # If the word count is too low, assume it's a scanned PDF and use OCR\n",
        "        if word_count < 100:  # Threshold to determine if OCR is needed\n",
        "            word_count = count_words_in_scanned_pdf(file_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path} with PyMuPDF, using OCR instead: {e}\")\n",
        "        word_count = count_words_in_scanned_pdf(file_path)\n",
        "\n",
        "    return word_count\n",
        "\n",
        "# Function to count in-text citations in a PDF using pdfminer\n",
        "def count_in_text_citations_pdfminer(file_path):\n",
        "    total_citations = 0\n",
        "    citations_per_200_words = 0\n",
        "\n",
        "    # Extract text using pdfminer\n",
        "    text = extract_text(file_path)\n",
        "\n",
        "    if text:\n",
        "        # Improved regex patterns to match various in-text citation formats\n",
        "        citation_patterns = [\n",
        "            r\"\\b([A-Z][a-zA-Z'`-]+, \\d{4})\\b\",                        # Author, Year\n",
        "            r\"\\b([A-Z][a-zA-Z'`-]+ et al\\., \\d{4})\\b\",                 # Author et al., Year\n",
        "            r\"\\b([A-Z][a-zA-Z'`-]+ & [A-Z][a-zA-Z'`-]+, \\d{4})\\b\",     # Author & Author, Year\n",
        "            r\"\\(\\s*[A-Z][a-zA-Z'`-]+, \\d{4}\\s*\\)\",                     # (Author, Year)\n",
        "            r\"\\(\\s*[A-Z][a-zA-Z'`-]+ et al\\., \\d{4}\\s*\\)\",             # (Author et al., Year)\n",
        "            r\"\\(\\s*[A-Z][a-zA-Z'`-]+ & [A-Z][a-zA-Z'`-]+, \\d{4}\\s*\\)\"  # (Author & Author, Year)\n",
        "        ]\n",
        "\n",
        "        citations = []\n",
        "        for pattern in citation_patterns:\n",
        "            citations.extend(re.findall(pattern, text))\n",
        "\n",
        "        total_citations = len(citations)\n",
        "\n",
        "        # Calculate citations per 200 words\n",
        "        word_count = len(text.split())\n",
        "        if word_count > 0:\n",
        "            citations_per_200_words = (total_citations / word_count) * 200\n",
        "\n",
        "    return total_citations, citations_per_200_words\n",
        "\n",
        "# Define paths for the datasets manually mounted in Google Colab\n",
        "dataset_1_path = '/content/drive/MyDrive/dataset_1.pdf'  # Path to Dataset 1 (Dissertations)\n",
        "dataset_2_path = '/content/drive/MyDrive/dataset_2.pdf'  # Path to Dataset 2 (Peer-Reviewed Articles)\n",
        "\n",
        "# Count the total number of words in Dataset 1 and Dataset 2\n",
        "dataset_1_word_count = count_words_in_pdf(dataset_1_path)\n",
        "dataset_2_word_count = count_words_in_pdf(dataset_2_path)\n",
        "\n",
        "print(f\"Total number of words in Dataset 1 (Dissertations): {dataset_1_word_count}\")\n",
        "print(f\"Total number of words in Dataset 2 (Peer-Reviewed Articles): {dataset_2_word_count}\")\n",
        "\n",
        "# Count in-text citations for Dataset 1 and Dataset 2 using pdfminer\n",
        "dataset_1_citations, dataset_1_citations_per_200_words = count_in_text_citations_pdfminer(dataset_1_path)\n",
        "dataset_2_citations, dataset_2_citations_per_200_words = count_in_text_citations_pdfminer(dataset_2_path)\n",
        "\n",
        "print(f\"Total in-text citations in Dataset 1 (Dissertations): {dataset_1_citations}\")\n",
        "print(f\"In-text citations per 200 words in Dataset 1 (Dissertations): {dataset_1_citations_per_200_words}\")\n",
        "print(f\"Total in-text citations in Dataset 2 (Peer-Reviewed Articles): {dataset_2_citations}\")\n",
        "print(f\"In-text citations per 200 words in Dataset 2 (Peer-Reviewed Articles): {dataset_2_citations_per_200_words}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-_-qUyBFPk-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6YK1_tMFfoX",
        "outputId": "e7d8a944-c2bc-4541-d9fd-12ce5250f820"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence types in Dataset 1 (Dissertations): {'type_1': 0, 'type_2': 0}\n",
            "Sentence types in Dataset 2 (Peer-Reviewed Articles): {'type_1': 0, 'type_2': 0}\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from pdfminer.high_level import extract_text\n",
        "\n",
        "# Function to count types of sentence structures in a PDF\n",
        "def count_sentence_types(file_path):\n",
        "    # Extract text using pdfminer\n",
        "    text = extract_text(file_path)\n",
        "\n",
        "    # Patterns for different types of sentence structures\n",
        "    sentence_patterns = {\n",
        "        \"type_1\": r\"\\b(clause\\s+(and|or|nor|so|for|but|yet)\\s+clause)\\b\",  # Type 1: Clause + conjunction (and, or, nor, so, for, but, yet)\n",
        "        \"type_2\": r\"\\b(clause,\\s+(when|since|while|if|whenever|because|wherever|that|which)\\s+clause)\\b\"  # Type 2: Clause + subordinating conjunction\n",
        "    }\n",
        "\n",
        "    # Count occurrences of each sentence type\n",
        "    sentence_counts = {key: len(re.findall(pattern, text)) for key, pattern in sentence_patterns.items()}\n",
        "\n",
        "    return sentence_counts\n",
        "\n",
        "# Define paths for the datasets manually mounted in Google Colab\n",
        "dataset_1_path = '/content/drive/MyDrive/dataset_1.pdf'  # Path to Dataset 1 (Dissertations)\n",
        "dataset_2_path = '/content/drive/MyDrive/dataset_2.pdf'  # Path to Dataset 2 (Peer-Reviewed Articles)\n",
        "\n",
        "# Count the sentence types for Dataset 1 and Dataset 2\n",
        "dataset_1_sentence_types = count_sentence_types(dataset_1_path)\n",
        "dataset_2_sentence_types = count_sentence_types(dataset_2_path)\n",
        "\n",
        "print(f\"Sentence types in Dataset 1 (Dissertations): {dataset_1_sentence_types}\")\n",
        "print(f\"Sentence types in Dataset 2 (Peer-Reviewed Articles): {dataset_2_sentence_types}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArgpB_5mG02i",
        "outputId": "370191df-ecf5-420a-8cab-5719c7fa798c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence complexity in Dataset 1 (Dissertations): {'average_sentence_length': 15.555598907530237, 'average_clauses_per_sentence': 1.583008193523215}\n",
            "Sentence complexity in Dataset 2 (Peer-Reviewed Articles): {'average_sentence_length': 15.280026990553306, 'average_clauses_per_sentence': 1.6360773729194782}\n",
            "Dataset 2 (Peer-Reviewed Articles) has more complex sentences.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from pdfminer.high_level import extract_text\n",
        "import spacy\n",
        "\n",
        "# Load spaCy's English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Increase spaCy's max length limit to handle larger texts\n",
        "nlp.max_length = 1500000\n",
        "\n",
        "# Function to determine sentence complexity using average sentence length and number of clauses\n",
        "def analyze_sentence_complexity(file_path):\n",
        "    # Extract text using pdfminer\n",
        "    text = extract_text(file_path)\n",
        "\n",
        "    # Process the text using spaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    total_sentences = 0\n",
        "    total_words = 0\n",
        "    total_clauses = 0\n",
        "\n",
        "    for sent in doc.sents:\n",
        "        total_sentences += 1\n",
        "        words = [token.text for token in sent if token.is_alpha]\n",
        "        total_words += len(words)\n",
        "\n",
        "        # Count the number of clauses based on the number of verbs in the sentence\n",
        "        clauses = [token for token in sent if token.pos_ == \"VERB\"]\n",
        "        total_clauses += len(clauses)\n",
        "\n",
        "    # Calculate average sentence length and average clauses per sentence\n",
        "    avg_sentence_length = total_words / total_sentences if total_sentences > 0 else 0\n",
        "    avg_clauses_per_sentence = total_clauses / total_sentences if total_sentences > 0 else 0\n",
        "\n",
        "    return {\n",
        "        \"average_sentence_length\": avg_sentence_length,\n",
        "        \"average_clauses_per_sentence\": avg_clauses_per_sentence\n",
        "    }\n",
        "\n",
        "# Define paths for the datasets manually mounted in Google Colab\n",
        "dataset_1_path = '/content/drive/MyDrive/dataset_1.pdf'  # Path to Dataset 1 (Dissertations)\n",
        "dataset_2_path = '/content/drive/MyDrive/dataset_2.pdf'  # Path to Dataset 2 (Peer-Reviewed Articles)\n",
        "\n",
        "# Analyze sentence complexity for Dataset 1 and Dataset 2\n",
        "dataset_1_complexity = analyze_sentence_complexity(dataset_1_path)\n",
        "dataset_2_complexity = analyze_sentence_complexity(dataset_2_path)\n",
        "\n",
        "print(f\"Sentence complexity in Dataset 1 (Dissertations): {dataset_1_complexity}\")\n",
        "print(f\"Sentence complexity in Dataset 2 (Peer-Reviewed Articles): {dataset_2_complexity}\")\n",
        "\n",
        "# Determine which dataset has more complex sentences\n",
        "if dataset_1_complexity[\"average_clauses_per_sentence\"] > dataset_2_complexity[\"average_clauses_per_sentence\"]:\n",
        "    print(\"Dataset 1 (Dissertations) has more complex sentences.\")\n",
        "elif dataset_1_complexity[\"average_clauses_per_sentence\"] < dataset_2_complexity[\"average_clauses_per_sentence\"]:\n",
        "    print(\"Dataset 2 (Peer-Reviewed Articles) has more complex sentences.\")\n",
        "else:\n",
        "    print(\"Both datasets have similar sentence complexity.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfminer.six\n",
        "from pdfminer.high_level import extract_text\n",
        "import spacy\n",
        "\n",
        "# Load spaCy's English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Increase spaCy's max length limit to handle larger texts\n",
        "nlp.max_length = 1500000\n",
        "\n",
        "# Function to analyze content density by counting ideas and new concepts\n",
        "def analyze_content_density(file_path):\n",
        "    # Extract text using pdfminer\n",
        "    text = extract_text(file_path)\n",
        "\n",
        "    # Split the text into manageable chunks to avoid excessive processing time\n",
        "    text_chunks = [text[i:i+100000] for i in range(0, len(text), 100000)]\n",
        "\n",
        "    total_ideas = 0\n",
        "    new_concepts = set()\n",
        "\n",
        "    # Process each chunk separately to improve performance\n",
        "    for chunk in text_chunks:\n",
        "        doc = nlp(chunk)\n",
        "\n",
        "        # Iterate through sentences to identify ideas and concepts\n",
        "        for sent in doc.sents:\n",
        "            # Consider each sentence as a potential new idea\n",
        "            total_ideas += 1\n",
        "\n",
        "            # Identify nouns and noun phrases to count as new concepts\n",
        "            for noun_chunk in sent.noun_chunks:\n",
        "                concept = noun_chunk.text.lower()\n",
        "                new_concepts.add(concept)\n",
        "\n",
        "    total_new_concepts = len(new_concepts)\n",
        "\n",
        "    return {\n",
        "        \"total_ideas\": total_ideas,\n",
        "        \"total_new_concepts\": total_new_concepts\n",
        "    }\n",
        "\n",
        "# Define paths for the datasets manually mounted in Google Colab\n",
        "dataset_1_path = '/content/drive/MyDrive/dataset_1.pdf'  # Path to Dataset 1 (Dissertations)\n",
        "dataset_2_path = '/content/drive/MyDrive/dataset_2.pdf'  # Path to Dataset 2 (Peer-Reviewed Articles)\n",
        "\n",
        "# Analyze content density for Dataset 1 and Dataset 2\n",
        "dataset_1_density = analyze_content_density(dataset_1_path)\n",
        "dataset_2_density = analyze_content_density(dataset_2_path)\n",
        "\n",
        "print(f\"Content density in Dataset 1 (Dissertations): {dataset_1_density}\")\n",
        "print(f\"Content density in Dataset 2 (Peer-Reviewed Articles): {dataset_2_density}\")\n",
        "\n",
        "# Output results in the required format\n",
        "print(\"\\nSheet 5: Content Density Analysis\")\n",
        "print(\"Dataset Type\\tTotal Number of Ideas\\tTotal Number of New Concepts\")\n",
        "print(f\"Dataset 1 (Dissertations)\\t{dataset_1_density['total_ideas']}\\t{dataset_1_density['total_new_concepts']}\")\n",
        "print(f\"Dataset 2 (Articles)\\t{dataset_2_density['total_ideas']}\\t{dataset_2_density['total_new_concepts']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9N3qSb0WYIm",
        "outputId": "348d32b3-7230-4166-8890-ee3a29562547"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.10/dist-packages (20240706)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.4.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Content density in Dataset 1 (Dissertations): {'total_ideas': 10265, 'total_new_concepts': 24744}\n",
            "Content density in Dataset 2 (Peer-Reviewed Articles): {'total_ideas': 4450, 'total_new_concepts': 11575}\n",
            "\n",
            "Sheet 5: Content Density Analysis\n",
            "Dataset Type\tTotal Number of Ideas\tTotal Number of New Concepts\n",
            "Dataset 1 (Dissertations)\t10265\t24744\n",
            "Dataset 2 (Articles)\t4450\t11575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pdfminer.high_level import extract_text\n",
        "import spacy\n",
        "\n",
        "# Load spaCy's English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Increase spaCy's max length limit to handle larger texts\n",
        "nlp.max_length = 1500000\n",
        "\n",
        "# Function to perform Ideational Meta-Function Analysis\n",
        "def analyze_ideational_meta_function(file_path):\n",
        "    # Extract text using pdfminer\n",
        "    text = extract_text(file_path)\n",
        "\n",
        "    # Split the text into manageable chunks to avoid excessive processing time\n",
        "    text_chunks = [text[i:i+100000] for i in range(0, len(text), 100000)]\n",
        "\n",
        "    material_processes = 0\n",
        "    mental_processes = 0\n",
        "    relational_processes = 0\n",
        "    participants_count = 0\n",
        "    circumstances_count = 0\n",
        "\n",
        "    # Process each chunk separately to improve performance\n",
        "    for chunk in text_chunks:\n",
        "        doc = nlp(chunk)\n",
        "\n",
        "        # Iterate through tokens to analyze ideational meta-function elements\n",
        "        for token in doc:\n",
        "            # Count processes based on verb type\n",
        "            if token.pos_ == \"VERB\":\n",
        "                if token.tag_ in [\"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]:  # General verb tags\n",
        "                    # Classify verb types as material, mental, or relational\n",
        "                    if token.lemma_ in [\"do\", \"make\", \"build\", \"create\", \"move\"]:\n",
        "                        material_processes += 1\n",
        "                    elif token.lemma_ in [\"think\", \"believe\", \"know\", \"feel\", \"understand\"]:\n",
        "                        mental_processes += 1\n",
        "                    elif token.lemma_ in [\"be\", \"seem\", \"become\", \"appear\"]:\n",
        "                        relational_processes += 1\n",
        "\n",
        "            # Count participants (nouns)\n",
        "            if token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\":\n",
        "                participants_count += 1\n",
        "\n",
        "            # Count circumstances (adverbs and prepositional phrases)\n",
        "            if token.pos_ == \"ADV\" or token.dep_ == \"prep\":\n",
        "                circumstances_count += 1\n",
        "\n",
        "    return {\n",
        "        \"material_processes\": material_processes,\n",
        "        \"mental_processes\": mental_processes,\n",
        "        \"relational_processes\": relational_processes,\n",
        "        \"participants_count\": participants_count,\n",
        "        \"circumstances_count\": circumstances_count\n",
        "    }\n",
        "\n",
        "# Define paths for the datasets manually mounted in Google Colab\n",
        "dataset_1_path = '/content/drive/MyDrive/dataset_1.pdf'  # Path to Dataset 1 (Dissertations)\n",
        "dataset_2_path = '/content/drive/MyDrive/dataset_2.pdf'  # Path to Dataset 2 (Peer-Reviewed Articles)\n",
        "\n",
        "# Analyze ideational meta-function for Dataset 1 and Dataset 2\n",
        "dataset_1_ideational = analyze_ideational_meta_function(dataset_1_path)\n",
        "dataset_2_ideational = analyze_ideational_meta_function(dataset_2_path)\n",
        "\n",
        "print(f\"Ideational Meta-Function Analysis for Dataset 1 (Dissertations): {dataset_1_ideational}\")\n",
        "print(f\"Ideational Meta-Function Analysis for Dataset 2 (Peer-Reviewed Articles): {dataset_2_ideational}\")\n",
        "\n",
        "# Output results in the required format\n",
        "print(\"\\nSheet 6: Ideational Meta-Function Analysis\")\n",
        "print(\"Dataset Type\\tProcesses (Material)\\tProcesses (Mental)\\tProcesses (Relational)\\tParticipants Count\\tCircumstances Count\")\n",
        "print(f\"Dataset 1 (Dissertations)\\t{dataset_1_ideational['material_processes']}\\t{dataset_1_ideational['mental_processes']}\\t{dataset_1_ideational['relational_processes']}\\t{dataset_1_ideational['participants_count']}\\t{dataset_1_ideational['circumstances_count']}\")\n",
        "print(f\"Dataset 2 (Articles)\\t{dataset_2_ideational['material_processes']}\\t{dataset_2_ideational['mental_processes']}\\t{dataset_2_ideational['relational_processes']}\\t{dataset_2_ideational['participants_count']}\\t{dataset_2_ideational['circumstances_count']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uS12bwG9YKQe",
        "outputId": "45562ae3-f6d5-4d27-a76f-c48e56116db0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ideational Meta-Function Analysis for Dataset 1 (Dissertations): {'material_processes': 475, 'mental_processes': 147, 'relational_processes': 247, 'participants_count': 72931, 'circumstances_count': 23100}\n",
            "Ideational Meta-Function Analysis for Dataset 2 (Peer-Reviewed Articles): {'material_processes': 269, 'mental_processes': 122, 'relational_processes': 111, 'participants_count': 32941, 'circumstances_count': 8994}\n",
            "\n",
            "Sheet 6: Ideational Meta-Function Analysis\n",
            "Dataset Type\tProcesses (Material)\tProcesses (Mental)\tProcesses (Relational)\tParticipants Count\tCircumstances Count\n",
            "Dataset 1 (Dissertations)\t475\t147\t247\t72931\t23100\n",
            "Dataset 2 (Articles)\t269\t122\t111\t32941\t8994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pdfminer.high_level import extract_text\n",
        "import spacy\n",
        "\n",
        "# Load spaCy's English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Increase spaCy's max length limit to handle larger texts\n",
        "nlp.max_length = 1500000\n",
        "\n",
        "# Function to perform Interpersonal Meta-Function Analysis\n",
        "def analyze_interpersonal_meta_function(file_path):\n",
        "    # Extract text using pdfminer\n",
        "    text = extract_text(file_path)\n",
        "\n",
        "    # Split the text into manageable chunks to avoid excessive processing time\n",
        "    text_chunks = [text[i:i+100000] for i in range(0, len(text), 100000)]\n",
        "\n",
        "    declarative_clauses = 0\n",
        "    interrogative_clauses = 0\n",
        "    imperative_clauses = 0\n",
        "    modality_count = 0\n",
        "    pronouns_terms_count = 0\n",
        "\n",
        "    # List of common modal verbs\n",
        "    modal_verbs = [\"can\", \"could\", \"will\", \"would\", \"shall\", \"should\", \"may\", \"might\", \"must\"]\n",
        "\n",
        "    # Process each chunk separately to improve performance\n",
        "    for chunk in text_chunks:\n",
        "        doc = nlp(chunk)\n",
        "\n",
        "        # Iterate through sentences to analyze clause types and modality\n",
        "        for sent in doc.sents:\n",
        "            # Determine clause type\n",
        "            if any(token.tag_ == \"VBP\" or token.tag_ == \"VBZ\" for token in sent):\n",
        "                declarative_clauses += 1\n",
        "            elif any(token.tag_ == \"MD\" and token.text.lower() in modal_verbs for token in sent):\n",
        "                interrogative_clauses += 1\n",
        "            elif any(token.dep_ == \"ROOT\" and token.pos_ == \"VERB\" and token.tag_ == \"VB\" for token in sent):\n",
        "                imperative_clauses += 1\n",
        "\n",
        "            # Count modality (e.g., can, must, might)\n",
        "            modality_count += sum(1 for token in sent if token.text.lower() in modal_verbs)\n",
        "\n",
        "            # Count pronouns and terms of address\n",
        "            pronouns_terms_count += sum(1 for token in sent if token.pos_ == \"PRON\")\n",
        "\n",
        "    return {\n",
        "        \"declarative_clauses\": declarative_clauses,\n",
        "        \"interrogative_clauses\": interrogative_clauses,\n",
        "        \"imperative_clauses\": imperative_clauses,\n",
        "        \"modality_count\": modality_count,\n",
        "        \"pronouns_terms_count\": pronouns_terms_count\n",
        "    }\n",
        "\n",
        "# Define paths for the datasets manually mounted in Google Colab\n",
        "dataset_1_path = '/content/drive/MyDrive/dataset_1.pdf'  # Path to Dataset 1 (Dissertations)\n",
        "dataset_2_path = '/content/drive/MyDrive/dataset_2.pdf'  # Path to Dataset 2 (Peer-Reviewed Articles)\n",
        "\n",
        "# Analyze interpersonal meta-function for Dataset 1 and Dataset 2\n",
        "dataset_1_interpersonal = analyze_interpersonal_meta_function(dataset_1_path)\n",
        "dataset_2_interpersonal = analyze_interpersonal_meta_function(dataset_2_path)\n",
        "\n",
        "print(f\"Interpersonal Meta-Function Analysis for Dataset 1 (Dissertations): {dataset_1_interpersonal}\")\n",
        "print(f\"Interpersonal Meta-Function Analysis for Dataset 2 (Peer-Reviewed Articles): {dataset_2_interpersonal}\")\n",
        "\n",
        "# Output results in the required format\n",
        "print(\"\\nSheet 7: Interpersonal Meta-Function Analysis\")\n",
        "print(\"Dataset Type\\tDeclarative Clauses\\tInterrogative Clauses\\tImperative Clauses\\tModality Count\\tPronouns & Terms Count\")\n",
        "print(f\"Dataset 1 (Dissertations)\\t{dataset_1_interpersonal['declarative_clauses']}\\t{dataset_1_interpersonal['interrogative_clauses']}\\t{dataset_1_interpersonal['imperative_clauses']}\\t{dataset_1_interpersonal['modality_count']}\\t{dataset_1_interpersonal['pronouns_terms_count']}\")\n",
        "print(f\"Dataset 2 (Articles)\\t{dataset_2_interpersonal['declarative_clauses']}\\t{dataset_2_interpersonal['interrogative_clauses']}\\t{dataset_2_interpersonal['imperative_clauses']}\\t{dataset_2_interpersonal['modality_count']}\\t{dataset_2_interpersonal['pronouns_terms_count']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4uZ3J0RY1P3",
        "outputId": "24a8fc3d-5e0b-4302-970b-508b5bef44a2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Interpersonal Meta-Function Analysis for Dataset 1 (Dissertations): {'declarative_clauses': 4801, 'interrogative_clauses': 564, 'imperative_clauses': 70, 'modality_count': 1432, 'pronouns_terms_count': 4310}\n",
            "Interpersonal Meta-Function Analysis for Dataset 2 (Peer-Reviewed Articles): {'declarative_clauses': 1770, 'interrogative_clauses': 239, 'imperative_clauses': 92, 'modality_count': 512, 'pronouns_terms_count': 1884}\n",
            "\n",
            "Sheet 7: Interpersonal Meta-Function Analysis\n",
            "Dataset Type\tDeclarative Clauses\tInterrogative Clauses\tImperative Clauses\tModality Count\tPronouns & Terms Count\n",
            "Dataset 1 (Dissertations)\t4801\t564\t70\t1432\t4310\n",
            "Dataset 2 (Articles)\t1770\t239\t92\t512\t1884\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pdfminer.high_level import extract_text\n",
        "import spacy\n",
        "\n",
        "# Load spaCy's English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Increase spaCy's max length limit to handle larger texts\n",
        "nlp.max_length = 1500000\n",
        "\n",
        "# Function to perform Textual Meta-Function Analysis\n",
        "def analyze_textual_meta_function(file_path):\n",
        "    # Extract text using pdfminer\n",
        "    text = extract_text(file_path)\n",
        "\n",
        "    # Split the text into manageable chunks to avoid excessive processing time\n",
        "    text_chunks = [text[i:i+100000] for i in range(0, len(text), 100000)]\n",
        "\n",
        "    theme_rheme_count = 0\n",
        "    conjunction_count = 0\n",
        "    reference_words_count = 0\n",
        "    substitution_count = 0\n",
        "    ellipses_count = 0\n",
        "    lexical_chains_count = 0\n",
        "\n",
        "    # List of common conjunctions\n",
        "    conjunctions = [\"and\", \"or\", \"but\", \"so\", \"because\", \"although\", \"if\", \"when\", \"while\"]\n",
        "\n",
        "    # Process each chunk separately to improve performance\n",
        "    for chunk in text_chunks:\n",
        "        doc = nlp(chunk)\n",
        "\n",
        "        # Iterate through sentences to analyze textual meta-function elements\n",
        "        for sent in doc.sents:\n",
        "            # Count theme and rheme (simplified as counting sentences)\n",
        "            theme_rheme_count += 1\n",
        "\n",
        "            # Count conjunctions\n",
        "            conjunction_count += sum(1 for token in sent if token.text.lower() in conjunctions)\n",
        "\n",
        "            # Count reference words (e.g., pronouns)\n",
        "            reference_words_count += sum(1 for token in sent if token.pos_ == \"PRON\")\n",
        "\n",
        "            # Count substitutions (e.g., \"one\", \"do so\")\n",
        "            substitution_count += sum(1 for token in sent if token.lemma_ in [\"one\", \"do\", \"so\"])\n",
        "\n",
        "            # Count ellipses (simplified as counting \"...\")\n",
        "            ellipses_count += sent.text.count(\"...\")\n",
        "\n",
        "        # Lexical chains (simplified as counting noun phrases)\n",
        "        lexical_chains_count += sum(1 for chunk in doc.noun_chunks)\n",
        "\n",
        "    return {\n",
        "        \"theme_rheme_count\": theme_rheme_count,\n",
        "        \"conjunction_count\": conjunction_count,\n",
        "        \"reference_words_count\": reference_words_count,\n",
        "        \"substitution_count\": substitution_count,\n",
        "        \"ellipses_count\": ellipses_count,\n",
        "        \"lexical_chains_count\": lexical_chains_count\n",
        "    }\n",
        "\n",
        "# Define paths for the datasets manually mounted in Google Colab\n",
        "dataset_1_path = '/content/drive/MyDrive/dataset_1.pdf'  # Path to Dataset 1 (Dissertations)\n",
        "dataset_2_path = '/content/drive/MyDrive/dataset_2.pdf'  # Path to Dataset 2 (Peer-Reviewed Articles)\n",
        "\n",
        "# Analyze textual meta-function for Dataset 1 and Dataset 2\n",
        "dataset_1_textual = analyze_textual_meta_function(dataset_1_path)\n",
        "dataset_2_textual = analyze_textual_meta_function(dataset_2_path)\n",
        "\n",
        "print(f\"Textual Meta-Function Analysis for Dataset 1 (Dissertations): {dataset_1_textual}\")\n",
        "print(f\"Textual Meta-Function Analysis for Dataset 2 (Peer-Reviewed Articles): {dataset_2_textual}\")\n",
        "\n",
        "# Output results in the required format\n",
        "print(\"\\nSheet 8: Textual Meta-Function Analysis\")\n",
        "print(\"Dataset Type\\tTheme & Rheme Count\\tConjunction Count\\tReference Words Count\\tSubstitution Count\\tEllipses Count\\tLexical Chains Count\")\n",
        "print(f\"Dataset 1 (Dissertations)\\t{dataset_1_textual['theme_rheme_count']}\\t{dataset_1_textual['conjunction_count']}\\t{dataset_1_textual['reference_words_count']}\\t{dataset_1_textual['substitution_count']}\\t{dataset_1_textual['ellipses_count']}\\t{dataset_1_textual['lexical_chains_count']}\")\n",
        "print(f\"Dataset 2 (Articles)\\t{dataset_2_textual['theme_rheme_count']}\\t{dataset_2_textual['conjunction_count']}\\t{dataset_2_textual['reference_words_count']}\\t{dataset_2_textual['substitution_count']}\\t{dataset_2_textual['ellipses_count']}\\t{dataset_2_textual['lexical_chains_count']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNWcyPO7ZkA2",
        "outputId": "7984e8eb-fb8b-4d61-97b9-35b3f0488ceb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Textual Meta-Function Analysis for Dataset 1 (Dissertations): {'theme_rheme_count': 10265, 'conjunction_count': 7516, 'reference_words_count': 4310, 'substitution_count': 385, 'ellipses_count': 23919, 'lexical_chains_count': 51685}\n",
            "Textual Meta-Function Analysis for Dataset 2 (Peer-Reviewed Articles): {'theme_rheme_count': 4450, 'conjunction_count': 3318, 'reference_words_count': 1884, 'substitution_count': 256, 'ellipses_count': 0, 'lexical_chains_count': 22184}\n",
            "\n",
            "Sheet 8: Textual Meta-Function Analysis\n",
            "Dataset Type\tTheme & Rheme Count\tConjunction Count\tReference Words Count\tSubstitution Count\tEllipses Count\tLexical Chains Count\n",
            "Dataset 1 (Dissertations)\t10265\t7516\t4310\t385\t23919\t51685\n",
            "Dataset 2 (Articles)\t4450\t3318\t1884\t256\t0\t22184\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1-VkN7tmcT3iDwRn0MAgmI1mxzPQZin4g",
      "authorship_tag": "ABX9TyPGEppFcsMbIPQFdjFTkTQ4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}