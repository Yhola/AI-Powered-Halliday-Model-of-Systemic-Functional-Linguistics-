{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yhola/AI-Powered-Halliday-Model-of-Systemic-Functional-Linguistics-/blob/main/Untitled14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJgKvY9V7S71",
        "outputId": "2ef16acc-ff7d-463f-cf4a-602c6be92c83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement re (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for re\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.6)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries for the project\n",
        "!pip install PyPDF2\n",
        "!pip install nltk\n",
        "!pip install scikit-learn\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install re\n",
        "!pip install spacy\n",
        "!pip install textblob\n",
        "\n",
        "# Additional setup\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import spacy\n",
        "# Download SpaCy model for NLP tasks\n",
        "!python -m spacy download en_core_web_sm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "cGxGjwOS9PhE",
        "outputId": "faa38dfe-f9ef-4226-a0e9-0ecd7b754b68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upload Dataset 1 (Dissertations)\n",
            "Please upload the PDFs for the dataset.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-208e2067-8b09-4334-839b-71493dd62c4c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-208e2067-8b09-4334-839b-71493dd62c4c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving dataset_1.pdf to dataset_1.pdf\n"
          ]
        },
        {
          "ename": "DeprecationError",
          "evalue": "PdfFileReader is deprecated and was removed in PyPDF2 3.0.0. Use PdfReader instead.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDeprecationError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-51c2f24e6b19>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Upload Dataset 1 (Dissertations)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mdataset_1_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupload_pdfs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mdataset_1_word_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_words_in_uploaded_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_1_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Upload Dataset 2 (Peer-Reviewed Articles)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-51c2f24e6b19>\u001b[0m in \u001b[0;36mcount_words_in_uploaded_files\u001b[0;34m(uploaded_files)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;31m# Open and read the PDF file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpdf_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                 \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyPDF2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPdfFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m                 \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PyPDF2/_reader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1972\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mPdfFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPdfReader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1973\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1974\u001b[0;31m         \u001b[0mdeprecation_with_replacement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PdfFileReader\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"PdfReader\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"3.0.0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1975\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"strict\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1976\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# maintain the default\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PyPDF2/_utils.py\u001b[0m in \u001b[0;36mdeprecation_with_replacement\u001b[0;34m(old_name, new_name, removed_in)\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0mRaise\u001b[0m \u001b[0man\u001b[0m \u001b[0mexception\u001b[0m \u001b[0mthat\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0mwas\u001b[0m \u001b[0malready\u001b[0m \u001b[0mremoved\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbut\u001b[0m \u001b[0mhas\u001b[0m \u001b[0ma\u001b[0m \u001b[0mreplacement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m     \"\"\"\n\u001b[0;32m--> 369\u001b[0;31m     \u001b[0mdeprecation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEPR_MSG_HAPPENED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremoved_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PyPDF2/_utils.py\u001b[0m in \u001b[0;36mdeprecation\u001b[0;34m(msg)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdeprecation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mDeprecationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDeprecationError\u001b[0m: PdfFileReader is deprecated and was removed in PyPDF2 3.0.0. Use PdfReader instead."
          ]
        }
      ],
      "source": [
        "import PyPDF2\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Function to manually upload PDFs from the computer\n",
        "def upload_pdfs():\n",
        "    print(\"Please upload the PDFs for the dataset.\")\n",
        "    uploaded = files.upload()\n",
        "    uploaded_files = list(uploaded.keys())\n",
        "    return uploaded_files\n",
        "\n",
        "def count_words_in_uploaded_files(uploaded_files):\n",
        "    total_word_count = 0\n",
        "\n",
        "    # Loop through all uploaded PDF files\n",
        "    for file_name in uploaded_files:\n",
        "        if file_name.endswith('.pdf'):\n",
        "            # Open and read the PDF file\n",
        "            with open(file_name, 'rb') as pdf_file:\n",
        "                reader = PyPDF2.PdfFileReader(pdf_file)\n",
        "                text = \"\"\n",
        "\n",
        "                # Extract text from each page of the PDF\n",
        "                for page_num in range(reader.numPages):\n",
        "                    page = reader.getPage(page_num)\n",
        "                    text += page.extract_text() + \" \"\n",
        "\n",
        "                # Count words in the extracted text\n",
        "                word_count = len(text.split())\n",
        "                total_word_count += word_count\n",
        "\n",
        "    return total_word_count\n",
        "\n",
        "# Upload Dataset 1 (Dissertations)\n",
        "print(\"Upload Dataset 1 (Dissertations)\")\n",
        "dataset_1_files = upload_pdfs()\n",
        "dataset_1_word_count = count_words_in_uploaded_files(dataset_1_files)\n",
        "\n",
        "# Upload Dataset 2 (Peer-Reviewed Articles)\n",
        "print(\"Upload Dataset 2 (Peer-Reviewed Articles)\")\n",
        "dataset_2_files = upload_pdfs()\n",
        "dataset_2_word_count = count_words_in_uploaded_files(dataset_2_files)\n",
        "\n",
        "print(f\"Total number of words in Dataset 1 (Dissertations): {dataset_1_word_count}\")\n",
        "print(f\"Total number of words in Dataset 2 (Peer-Reviewed Articles): {dataset_2_word_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "HjqPlQteAPUq",
        "outputId": "53b4fadf-b0c8-4c99-84f6-f24d3deaea90"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'fitz'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-3bb9de81dd8e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfitz\u001b[0m  \u001b[0;31m# PyMuPDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpytesseract\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpdf2image\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_from_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fitz'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import fitz  # PyMuPDF\n",
        "import pytesseract\n",
        "from pdf2image import convert_from_path\n",
        "import os\n",
        "\n",
        "# Function to count words in a PDF using PyMuPDF (Fitz)\n",
        "def count_words_in_pdf_pymupdf(file_path):\n",
        "    total_word_count = 0\n",
        "\n",
        "    # Open the PDF with PyMuPDF\n",
        "    with fitz.open(file_path) as pdf:\n",
        "        for page_num in range(len(pdf)):\n",
        "            text = pdf.load_page(page_num).get_text()\n",
        "            total_word_count += len(text.split())\n",
        "\n",
        "    return total_word_count\n",
        "\n",
        "# Function to count words in scanned PDFs using OCR\n",
        "def count_words_in_scanned_pdf(file_path):\n",
        "    total_word_count = 0\n",
        "\n",
        "    # Convert PDF pages to images\n",
        "    images = convert_from_path(file_path)\n",
        "\n",
        "    for image in images:\n",
        "        # Use Tesseract to extract text from each image\n",
        "        text = pytesseract.image_to_string(image)\n",
        "        total_word_count += len(text.split())\n",
        "\n",
        "    return total_word_count\n",
        "\n",
        "# Function to decide whether to use PyMuPDF or OCR for word counting\n",
        "def count_words_in_pdf(file_path):\n",
        "    try:\n",
        "        # Try extracting text using PyMuPDF first\n",
        "        word_count = count_words_in_pdf_pymupdf(file_path)\n",
        "\n",
        "        # If the word count is too low, assume it's a scanned PDF and use OCR\n",
        "        if word_count < 100:  # Threshold to determine if OCR is needed\n",
        "            word_count = count_words_in_scanned_pdf(file_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path} with PyMuPDF, using OCR instead: {e}\")\n",
        "        word_count = count_words_in_scanned_pdf(file_path)\n",
        "\n",
        "    return word_count\n",
        "\n",
        "# Define file paths\n",
        "dataset_1_path = \"/mnt/data/dataset_1.pdf\"\n",
        "dataset_2_path = \"/mnt/data/dataset_2.pdf\"\n",
        "\n",
        "# Count the total number of words in Dataset 1 and Dataset 2\n",
        "dataset_1_word_count = count_words_in_pdf(dataset_1_path)\n",
        "dataset_2_word_count = count_words_in_pdf(dataset_2_path)\n",
        "\n",
        "print(f\"Total number of words in Dataset 1 (Dissertations): {dataset_1_word_count}\")\n",
        "print(f\"Total number of words in Dataset 2 (Peer-Reviewed Articles): {dataset_2_word_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAqipEI2BFrb",
        "outputId": "157ea45b-52eb-4d49-b8d1-ccbcc7940065"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of words in Dataset 1 (Dissertations): 173859\n",
            "Total number of words in Dataset 2 (Peer-Reviewed Articles): 71779\n"
          ]
        }
      ],
      "source": [
        "import fitz  # PyMuPDF\n",
        "import pytesseract\n",
        "from pdf2image import convert_from_path\n",
        "import os\n",
        "\n",
        "# Function to count words in a PDF using PyMuPDF (Fitz)\n",
        "def count_words_in_pdf_pymupdf(file_path):\n",
        "    total_word_count = 0\n",
        "\n",
        "    # Open the PDF with PyMuPDF\n",
        "    with fitz.open(file_path) as pdf:\n",
        "        for page_num in range(len(pdf)):\n",
        "            text = pdf.load_page(page_num).get_text()\n",
        "            total_word_count += len(text.split())\n",
        "\n",
        "    return total_word_count\n",
        "\n",
        "# Function to count words in scanned PDFs using OCR\n",
        "def count_words_in_scanned_pdf(file_path):\n",
        "    total_word_count = 0\n",
        "\n",
        "    # Convert PDF pages to images\n",
        "    images = convert_from_path(file_path)\n",
        "\n",
        "    for image in images:\n",
        "        # Use Tesseract to extract text from each image\n",
        "        text = pytesseract.image_to_string(image)\n",
        "        total_word_count += len(text.split())\n",
        "\n",
        "    return total_word_count\n",
        "\n",
        "# Function to decide whether to use PyMuPDF or OCR for word counting\n",
        "def count_words_in_pdf(file_path):\n",
        "    try:\n",
        "        # Try extracting text using PyMuPDF first\n",
        "        word_count = count_words_in_pdf_pymupdf(file_path)\n",
        "\n",
        "        # If the word count is too low, assume it's a scanned PDF and use OCR\n",
        "        if word_count < 100:  # Threshold to determine if OCR is needed\n",
        "            word_count = count_words_in_scanned_pdf(file_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path} with PyMuPDF, using OCR instead: {e}\")\n",
        "        word_count = count_words_in_scanned_pdf(file_path)\n",
        "\n",
        "    return word_count\n",
        "\n",
        "# Define paths for the datasets manually mounted in Google Colab\n",
        "dataset_1_path = '/content/drive/MyDrive/dataset_1.pdf'  # Path to Dataset 1 (Dissertations)\n",
        "dataset_2_path = '/content/drive/MyDrive/dataset_2.pdf'  # Path to Dataset 2 (Peer-Reviewed Articles)\n",
        "\n",
        "# Count the total number of words in Dataset 1 and Dataset 2\n",
        "dataset_1_word_count = count_words_in_pdf(dataset_1_path)\n",
        "dataset_2_word_count = count_words_in_pdf(dataset_2_path)\n",
        "\n",
        "print(f\"Total number of words in Dataset 1 (Dissertations): {dataset_1_word_count}\")\n",
        "print(f\"Total number of words in Dataset 2 (Peer-Reviewed Articles): {dataset_2_word_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJmVFq_7Bpsx",
        "outputId": "7136537a-80b8-468f-8df7-c9368c05ef05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of words in Dataset 1 (Dissertations): 173859\n",
            "Total number of words in Dataset 2 (Peer-Reviewed Articles): 71779\n",
            "Total in-text citations in Dataset 1 (Dissertations): 1397\n",
            "In-text citations per 200 words in Dataset 1 (Dissertations): 1.6070493905981285\n",
            "Total in-text citations in Dataset 2 (Peer-Reviewed Articles): 404\n",
            "In-text citations per 200 words in Dataset 2 (Peer-Reviewed Articles): 1.1256774265453684\n"
          ]
        }
      ],
      "source": [
        "import fitz  # PyMuPDF\n",
        "import pytesseract\n",
        "from pdf2image import convert_from_path\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Function to count words in a PDF using PyMuPDF (Fitz)\n",
        "def count_words_in_pdf_pymupdf(file_path):\n",
        "    total_word_count = 0\n",
        "\n",
        "    # Open the PDF with PyMuPDF\n",
        "    with fitz.open(file_path) as pdf:\n",
        "        for page_num in range(len(pdf)):\n",
        "            text = pdf.load_page(page_num).get_text()\n",
        "            total_word_count += len(text.split())\n",
        "\n",
        "    return total_word_count\n",
        "\n",
        "# Function to count words in scanned PDFs using OCR\n",
        "def count_words_in_scanned_pdf(file_path):\n",
        "    total_word_count = 0\n",
        "\n",
        "    # Convert PDF pages to images\n",
        "    images = convert_from_path(file_path)\n",
        "\n",
        "    for image in images:\n",
        "        # Use Tesseract to extract text from each image\n",
        "        text = pytesseract.image_to_string(image)\n",
        "        total_word_count += len(text.split())\n",
        "\n",
        "    return total_word_count\n",
        "\n",
        "# Function to decide whether to use PyMuPDF or OCR for word counting\n",
        "def count_words_in_pdf(file_path):\n",
        "    try:\n",
        "        # Try extracting text using PyMuPDF first\n",
        "        word_count = count_words_in_pdf_pymupdf(file_path)\n",
        "\n",
        "        # If the word count is too low, assume it's a scanned PDF and use OCR\n",
        "        if word_count < 100:  # Threshold to determine if OCR is needed\n",
        "            word_count = count_words_in_scanned_pdf(file_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path} with PyMuPDF, using OCR instead: {e}\")\n",
        "        word_count = count_words_in_scanned_pdf(file_path)\n",
        "\n",
        "    return word_count\n",
        "\n",
        "# Function to count in-text citations in a PDF\n",
        "def count_in_text_citations(file_path):\n",
        "    total_citations = 0\n",
        "    citations_per_200_words = 0\n",
        "\n",
        "    # Open the PDF with PyMuPDF\n",
        "    with fitz.open(file_path) as pdf:\n",
        "        text = \"\"\n",
        "        for page_num in range(len(pdf)):\n",
        "            text += pdf.load_page(page_num).get_text() + \" \"\n",
        "\n",
        "        # Improved regex patterns to match various in-text citation formats\n",
        "        citation_patterns = [\n",
        "            r\"\\b([A-Z][a-zA-Z'`-]+, \\d{4})\\b\",                        # Author, Year\n",
        "            r\"\\b([A-Z][a-zA-Z'`-]+ et al\\., \\d{4})\\b\",                 # Author et al., Year\n",
        "            r\"\\b([A-Z][a-zA-Z'`-]+ & [A-Z][a-zA-Z'`-]+, \\d{4})\\b\",     # Author & Author, Year\n",
        "            r\"\\(\\s*[A-Z][a-zA-Z'`-]+, \\d{4}\\s*\\)\",                     # (Author, Year)\n",
        "            r\"\\(\\s*[A-Z][a-zA-Z'`-]+ et al\\., \\d{4}\\s*\\)\",             # (Author et al., Year)\n",
        "            r\"\\(\\s*[A-Z][a-zA-Z'`-]+ & [A-Z][a-zA-Z'`-]+, \\d{4}\\s*\\)\"  # (Author & Author, Year)\n",
        "        ]\n",
        "\n",
        "        citations = []\n",
        "        for pattern in citation_patterns:\n",
        "            citations.extend(re.findall(pattern, text))\n",
        "\n",
        "        total_citations = len(citations)\n",
        "\n",
        "        # Calculate citations per 200 words\n",
        "        word_count = len(text.split())\n",
        "        if word_count > 0:\n",
        "            citations_per_200_words = (total_citations / word_count) * 200\n",
        "\n",
        "    return total_citations, citations_per_200_words\n",
        "\n",
        "# Define paths for the datasets manually mounted in Google Colab\n",
        "dataset_1_path = '/content/drive/MyDrive/dataset_1.pdf'  # Path to Dataset 1 (Dissertations)\n",
        "dataset_2_path = '/content/drive/MyDrive/dataset_2.pdf'  # Path to Dataset 2 (Peer-Reviewed Articles)\n",
        "\n",
        "# Count the total number of words in Dataset 1 and Dataset 2\n",
        "dataset_1_word_count = count_words_in_pdf(dataset_1_path)\n",
        "dataset_2_word_count = count_words_in_pdf(dataset_2_path)\n",
        "\n",
        "print(f\"Total number of words in Dataset 1 (Dissertations): {dataset_1_word_count}\")\n",
        "print(f\"Total number of words in Dataset 2 (Peer-Reviewed Articles): {dataset_2_word_count}\")\n",
        "\n",
        "# Count in-text citations for Dataset 1 and Dataset 2\n",
        "dataset_1_citations, dataset_1_citations_per_200_words = count_in_text_citations(dataset_1_path)\n",
        "dataset_2_citations, dataset_2_citations_per_200_words = count_in_text_citations(dataset_2_path)\n",
        "\n",
        "print(f\"Total in-text citations in Dataset 1 (Dissertations): {dataset_1_citations}\")\n",
        "print(f\"In-text citations per 200 words in Dataset 1 (Dissertations): {dataset_1_citations_per_200_words}\")\n",
        "print(f\"Total in-text citations in Dataset 2 (Peer-Reviewed Articles): {dataset_2_citations}\")\n",
        "print(f\"In-text citations per 200 words in Dataset 2 (Peer-Reviewed Articles): {dataset_2_citations_per_200_words}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eilm-8ZKCFvh",
        "outputId": "83debf8e-38d1-44d7-dcea-43b3a7718ca2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of words in Dataset 1 (Dissertations): 173859\n",
            "Total number of words in Dataset 2 (Peer-Reviewed Articles): 71779\n",
            "Total in-text citations in Dataset 1 (Dissertations): 1392\n",
            "In-text citations per 200 words in Dataset 1 (Dissertations): 1.5840412396943437\n",
            "Total in-text citations in Dataset 2 (Peer-Reviewed Articles): 386\n",
            "In-text citations per 200 words in Dataset 2 (Peer-Reviewed Articles): 1.0201250049552704\n"
          ]
        }
      ],
      "source": [
        "import fitz  # PyMuPDF\n",
        "import pytesseract\n",
        "from pdf2image import convert_from_path\n",
        "import os\n",
        "import re\n",
        "import PyPDF2\n",
        "\n",
        "# Function to count words in a PDF using PyMuPDF (Fitz)\n",
        "def count_words_in_pdf_pymupdf(file_path):\n",
        "    total_word_count = 0\n",
        "\n",
        "    # Open the PDF with PyMuPDF\n",
        "    with fitz.open(file_path) as pdf:\n",
        "        for page_num in range(len(pdf)):\n",
        "            text = pdf.load_page(page_num).get_text()\n",
        "            if text:\n",
        "                total_word_count += len(text.split())\n",
        "\n",
        "    return total_word_count\n",
        "\n",
        "# Function to count words in a scanned PDF using OCR\n",
        "def count_words_in_scanned_pdf(file_path):\n",
        "    total_word_count = 0\n",
        "\n",
        "    # Convert PDF pages to images\n",
        "    images = convert_from_path(file_path)\n",
        "\n",
        "    for image in images:\n",
        "        # Use Tesseract to extract text from each image\n",
        "        text = pytesseract.image_to_string(image)\n",
        "        if text:\n",
        "            total_word_count += len(text.split())\n",
        "\n",
        "    return total_word_count\n",
        "\n",
        "# Function to decide whether to use PyMuPDF or OCR for word counting\n",
        "def count_words_in_pdf(file_path):\n",
        "    try:\n",
        "        # Try extracting text using PyMuPDF first\n",
        "        word_count = count_words_in_pdf_pymupdf(file_path)\n",
        "\n",
        "        # If the word count is too low, assume it's a scanned PDF and use OCR\n",
        "        if word_count < 100:  # Threshold to determine if OCR is needed\n",
        "            word_count = count_words_in_scanned_pdf(file_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path} with PyMuPDF, using OCR instead: {e}\")\n",
        "        word_count = count_words_in_scanned_pdf(file_path)\n",
        "\n",
        "    return word_count\n",
        "\n",
        "# Function to count in-text citations in a PDF using PyPDF2\n",
        "def count_in_text_citations_pypdf2(file_path):\n",
        "    total_citations = 0\n",
        "    citations_per_200_words = 0\n",
        "\n",
        "    # Open the PDF with PyPDF2\n",
        "    with open(file_path, 'rb') as pdf_file:\n",
        "        reader = PyPDF2.PdfReader(pdf_file)\n",
        "        text = \"\"\n",
        "        for page_num in range(len(reader.pages)):\n",
        "            page = reader.pages[page_num]\n",
        "            text += page.extract_text() + \" \"\n",
        "\n",
        "        # Improved regex patterns to match various in-text citation formats\n",
        "        citation_patterns = [\n",
        "            r\"\\b([A-Z][a-zA-Z'`-]+, \\d{4})\\b\",                        # Author, Year\n",
        "            r\"\\b([A-Z][a-zA-Z'`-]+ et al\\., \\d{4})\\b\",                 # Author et al., Year\n",
        "            r\"\\b([A-Z][a-zA-Z'`-]+ & [A-Z][a-zA-Z'`-]+, \\d{4})\\b\",     # Author & Author, Year\n",
        "            r\"\\(\\s*[A-Z][a-zA-Z'`-]+, \\d{4}\\s*\\)\",                     # (Author, Year)\n",
        "            r\"\\(\\s*[A-Z][a-zA-Z'`-]+ et al\\., \\d{4}\\s*\\)\",             # (Author et al., Year)\n",
        "            r\"\\(\\s*[A-Z][a-zA-Z'`-]+ & [A-Z][a-zA-Z'`-]+, \\d{4}\\s*\\)\"  # (Author & Author, Year)\n",
        "        ]\n",
        "\n",
        "        citations = []\n",
        "        for pattern in citation_patterns:\n",
        "            citations.extend(re.findall(pattern, text))\n",
        "\n",
        "        total_citations = len(citations)\n",
        "\n",
        "        # Calculate citations per 200 words\n",
        "        word_count = len(text.split())\n",
        "        if word_count > 0:\n",
        "            citations_per_200_words = (total_citations / word_count) * 200\n",
        "\n",
        "    return total_citations, citations_per_200_words\n",
        "\n",
        "# Define paths for the datasets manually mounted in Google Colab\n",
        "dataset_1_path = '/content/drive/MyDrive/dataset_1.pdf'  # Path to Dataset 1 (Dissertations)\n",
        "dataset_2_path = '/content/drive/MyDrive/dataset_2.pdf'  # Path to Dataset 2 (Peer-Reviewed Articles)\n",
        "\n",
        "# Count the total number of words in Dataset 1 and Dataset 2\n",
        "dataset_1_word_count = count_words_in_pdf(dataset_1_path)\n",
        "dataset_2_word_count = count_words_in_pdf(dataset_2_path)\n",
        "\n",
        "print(f\"Total number of words in Dataset 1 (Dissertations): {dataset_1_word_count}\")\n",
        "print(f\"Total number of words in Dataset 2 (Peer-Reviewed Articles): {dataset_2_word_count}\")\n",
        "\n",
        "# Count in-text citations for Dataset 1 and Dataset 2\n",
        "dataset_1_citations, dataset_1_citations_per_200_words = count_in_text_citations_pypdf2(dataset_1_path)\n",
        "dataset_2_citations, dataset_2_citations_per_200_words = count_in_text_citations_pypdf2(dataset_2_path)\n",
        "\n",
        "print(f\"Total in-text citations in Dataset 1 (Dissertations): {dataset_1_citations}\")\n",
        "print(f\"In-text citations per 200 words in Dataset 1 (Dissertations): {dataset_1_citations_per_200_words}\")\n",
        "print(f\"Total in-text citations in Dataset 2 (Peer-Reviewed Articles): {dataset_2_citations}\")\n",
        "print(f\"In-text citations per 200 words in Dataset 2 (Peer-Reviewed Articles): {dataset_2_citations_per_200_words}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "jDyPWq7pDFYw",
        "outputId": "242bb262-a89f-4922-84d8-09c68b372266"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pdfminer'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-f29d32051095>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPyPDF2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpdfminer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh_level\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mextract_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Function to count words in a PDF using PyMuPDF (Fitz)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pdfminer'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import fitz  # PyMuPDF\n",
        "import pytesseract\n",
        "from pdf2image import convert_from_path\n",
        "import os\n",
        "import re\n",
        "import PyPDF2\n",
        "from pdfminer.high_level import extract_text\n",
        "\n",
        "# Function to count words in a PDF using PyMuPDF (Fitz)\n",
        "def count_words_in_pdf_pymupdf(file_path):\n",
        "    total_word_count = 0\n",
        "\n",
        "    # Open the PDF with PyMuPDF\n",
        "    with fitz.open(file_path) as pdf:\n",
        "        for page_num in range(len(pdf)):\n",
        "            text = pdf.load_page(page_num).get_text()\n",
        "            if text:\n",
        "                total_word_count += len(text.split())\n",
        "\n",
        "    return total_word_count\n",
        "\n",
        "# Function to count words in a scanned PDF using OCR\n",
        "def count_words_in_scanned_pdf(file_path):\n",
        "    total_word_count = 0\n",
        "\n",
        "    # Convert PDF pages to images\n",
        "    images = convert_from_path(file_path)\n",
        "\n",
        "    for image in images:\n",
        "        # Use Tesseract to extract text from each image\n",
        "        text = pytesseract.image_to_string(image)\n",
        "        if text:\n",
        "            total_word_count += len(text.split())\n",
        "\n",
        "    return total_word_count\n",
        "\n",
        "# Function to decide whether to use PyMuPDF or OCR for word counting\n",
        "def count_words_in_pdf(file_path):\n",
        "    try:\n",
        "        # Try extracting text using PyMuPDF first\n",
        "        word_count = count_words_in_pdf_pymupdf(file_path)\n",
        "\n",
        "        # If the word count is too low, assume it's a scanned PDF and use OCR\n",
        "        if word_count < 100:  # Threshold to determine if OCR is needed\n",
        "            word_count = count_words_in_scanned_pdf(file_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path} with PyMuPDF, using OCR instead: {e}\")\n",
        "        word_count = count_words_in_scanned_pdf(file_path)\n",
        "\n",
        "    return word_count\n",
        "\n",
        "# Function to count in-text citations in a PDF using pdfminer\n",
        "def count_in_text_citations_pdfminer(file_path):\n",
        "    total_citations = 0\n",
        "    citations_per_200_words = 0\n",
        "\n",
        "    # Extract text using pdfminer\n",
        "    text = extract_text(file_path)\n",
        "\n",
        "    if text:\n",
        "        # Improved regex patterns to match various in-text citation formats\n",
        "        citation_patterns = [\n",
        "            r\"\\b([A-Z][a-zA-Z'`-]+, \\d{4})\\b\",                        # Author, Year\n",
        "            r\"\\b([A-Z][a-zA-Z'`-]+ et al\\., \\d{4})\\b\",                 # Author et al., Year\n",
        "            r\"\\b([A-Z][a-zA-Z'`-]+ & [A-Z][a-zA-Z'`-]+, \\d{4})\\b\",     # Author & Author, Year\n",
        "            r\"\\(\\s*[A-Z][a-zA-Z'`-]+, \\d{4}\\s*\\)\",                     # (Author, Year)\n",
        "            r\"\\(\\s*[A-Z][a-zA-Z'`-]+ et al\\., \\d{4}\\s*\\)\",             # (Author et al., Year)\n",
        "            r\"\\(\\s*[A-Z][a-zA-Z'`-]+ & [A-Z][a-zA-Z'`-]+, \\d{4}\\s*\\)\"  # (Author & Author, Year)\n",
        "        ]\n",
        "\n",
        "        citations = []\n",
        "        for pattern in citation_patterns:\n",
        "            citations.extend(re.findall(pattern, text))\n",
        "\n",
        "        total_citations = len(citations)\n",
        "\n",
        "        # Calculate citations per 200 words\n",
        "        word_count = len(text.split())\n",
        "        if word_count > 0:\n",
        "            citations_per_200_words = (total_citations / word_count) * 200\n",
        "\n",
        "    return total_citations, citations_per_200_words\n",
        "\n",
        "# Define paths for the datasets manually mounted in Google Colab\n",
        "dataset_1_path = '/content/drive/MyDrive/dataset_1.pdf'  # Path to Dataset 1 (Dissertations)\n",
        "dataset_2_path = '/content/drive/MyDrive/dataset_2.pdf'  # Path to Dataset 2 (Peer-Reviewed Articles)\n",
        "\n",
        "# Count the total number of words in Dataset 1 and Dataset 2\n",
        "dataset_1_word_count = count_words_in_pdf(dataset_1_path)\n",
        "dataset_2_word_count = count_words_in_pdf(dataset_2_path)\n",
        "\n",
        "print(f\"Total number of words in Dataset 1 (Dissertations): {dataset_1_word_count}\")\n",
        "print(f\"Total number of words in Dataset 2 (Peer-Reviewed Articles): {dataset_2_word_count}\")\n",
        "\n",
        "# Count in-text citations for Dataset 1 and Dataset 2 using pdfminer\n",
        "dataset_1_citations, dataset_1_citations_per_200_words = count_in_text_citations_pdfminer(dataset_1_path)\n",
        "dataset_2_citations, dataset_2_citations_per_200_words = count_in_text_citations_pdfminer(dataset_2_path)\n",
        "\n",
        "print(f\"Total in-text citations in Dataset 1 (Dissertations): {dataset_1_citations}\")\n",
        "print(f\"In-text citations per 200 words in Dataset 1 (Dissertations): {dataset_1_citations_per_200_words}\")\n",
        "print(f\"Total in-text citations in Dataset 2 (Peer-Reviewed Articles): {dataset_2_citations}\")\n",
        "print(f\"In-text citations per 200 words in Dataset 2 (Peer-Reviewed Articles): {dataset_2_citations_per_200_words}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4mBVEjkDXFJ",
        "outputId": "6464d17f-e5ad-4a1e-b4e4-b46e8ef19439"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.4.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Downloading pdfminer.six-20240706-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pdfminer.six\n",
            "Successfully installed pdfminer.six-20240706\n",
            "Total number of words in Dataset 1 (Dissertations): 173859\n",
            "Total number of words in Dataset 2 (Peer-Reviewed Articles): 71779\n",
            "Total in-text citations in Dataset 1 (Dissertations): 1088\n",
            "In-text citations per 200 words in Dataset 1 (Dissertations): 1.2514593649533865\n",
            "Total in-text citations in Dataset 2 (Peer-Reviewed Articles): 324\n",
            "In-text citations per 200 words in Dataset 2 (Peer-Reviewed Articles): 0.8696935940624623\n"
          ]
        }
      ],
      "source": [
        "import fitz  # PyMuPDF\n",
        "import pytesseract\n",
        "from pdf2image import convert_from_path\n",
        "import os\n",
        "import re\n",
        "import PyPDF2\n",
        "\n",
        "# Install pdfminer package\n",
        "!pip install pdfminer.six\n",
        "from pdfminer.high_level import extract_text\n",
        "\n",
        "# Function to count words in a PDF using PyMuPDF (Fitz)\n",
        "def count_words_in_pdf_pymupdf(file_path):\n",
        "    total_word_count = 0\n",
        "\n",
        "    # Open the PDF with PyMuPDF\n",
        "    with fitz.open(file_path) as pdf:\n",
        "        for page_num in range(len(pdf)):\n",
        "            text = pdf.load_page(page_num).get_text()\n",
        "            if text:\n",
        "                total_word_count += len(text.split())\n",
        "\n",
        "    return total_word_count\n",
        "\n",
        "# Function to count words in a scanned PDF using OCR\n",
        "def count_words_in_scanned_pdf(file_path):\n",
        "    total_word_count = 0\n",
        "\n",
        "    # Convert PDF pages to images\n",
        "    images = convert_from_path(file_path)\n",
        "\n",
        "    for image in images:\n",
        "        # Use Tesseract to extract text from each image\n",
        "        text = pytesseract.image_to_string(image)\n",
        "        if text:\n",
        "            total_word_count += len(text.split())\n",
        "\n",
        "    return total_word_count\n",
        "\n",
        "# Function to decide whether to use PyMuPDF or OCR for word counting\n",
        "def count_words_in_pdf(file_path):\n",
        "    try:\n",
        "        # Try extracting text using PyMuPDF first\n",
        "        word_count = count_words_in_pdf_pymupdf(file_path)\n",
        "\n",
        "        # If the word count is too low, assume it's a scanned PDF and use OCR\n",
        "        if word_count < 100:  # Threshold to determine if OCR is needed\n",
        "            word_count = count_words_in_scanned_pdf(file_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path} with PyMuPDF, using OCR instead: {e}\")\n",
        "        word_count = count_words_in_scanned_pdf(file_path)\n",
        "\n",
        "    return word_count\n",
        "\n",
        "# Function to count in-text citations in a PDF using pdfminer\n",
        "def count_in_text_citations_pdfminer(file_path):\n",
        "    total_citations = 0\n",
        "    citations_per_200_words = 0\n",
        "\n",
        "    # Extract text using pdfminer\n",
        "    text = extract_text(file_path)\n",
        "\n",
        "    if text:\n",
        "        # Improved regex patterns to match various in-text citation formats\n",
        "        citation_patterns = [\n",
        "            r\"\\b([A-Z][a-zA-Z'`-]+, \\d{4})\\b\",                        # Author, Year\n",
        "            r\"\\b([A-Z][a-zA-Z'`-]+ et al\\., \\d{4})\\b\",                 # Author et al., Year\n",
        "            r\"\\b([A-Z][a-zA-Z'`-]+ & [A-Z][a-zA-Z'`-]+, \\d{4})\\b\",     # Author & Author, Year\n",
        "            r\"\\(\\s*[A-Z][a-zA-Z'`-]+, \\d{4}\\s*\\)\",                     # (Author, Year)\n",
        "            r\"\\(\\s*[A-Z][a-zA-Z'`-]+ et al\\., \\d{4}\\s*\\)\",             # (Author et al., Year)\n",
        "            r\"\\(\\s*[A-Z][a-zA-Z'`-]+ & [A-Z][a-zA-Z'`-]+, \\d{4}\\s*\\)\"  # (Author & Author, Year)\n",
        "        ]\n",
        "\n",
        "        citations = []\n",
        "        for pattern in citation_patterns:\n",
        "            citations.extend(re.findall(pattern, text))\n",
        "\n",
        "        total_citations = len(citations)\n",
        "\n",
        "        # Calculate citations per 200 words\n",
        "        word_count = len(text.split())\n",
        "        if word_count > 0:\n",
        "            citations_per_200_words = (total_citations / word_count) * 200\n",
        "\n",
        "    return total_citations, citations_per_200_words\n",
        "\n",
        "# Define paths for the datasets manually mounted in Google Colab\n",
        "dataset_1_path = '/content/drive/MyDrive/dataset_1.pdf'  # Path to Dataset 1 (Dissertations)\n",
        "dataset_2_path = '/content/drive/MyDrive/dataset_2.pdf'  # Path to Dataset 2 (Peer-Reviewed Articles)\n",
        "\n",
        "# Count the total number of words in Dataset 1 and Dataset 2\n",
        "dataset_1_word_count = count_words_in_pdf(dataset_1_path)\n",
        "dataset_2_word_count = count_words_in_pdf(dataset_2_path)\n",
        "\n",
        "print(f\"Total number of words in Dataset 1 (Dissertations): {dataset_1_word_count}\")\n",
        "print(f\"Total number of words in Dataset 2 (Peer-Reviewed Articles): {dataset_2_word_count}\")\n",
        "\n",
        "# Count in-text citations for Dataset 1 and Dataset 2 using pdfminer\n",
        "dataset_1_citations, dataset_1_citations_per_200_words = count_in_text_citations_pdfminer(dataset_1_path)\n",
        "dataset_2_citations, dataset_2_citations_per_200_words = count_in_text_citations_pdfminer(dataset_2_path)\n",
        "\n",
        "print(f\"Total in-text citations in Dataset 1 (Dissertations): {dataset_1_citations}\")\n",
        "print(f\"In-text citations per 200 words in Dataset 1 (Dissertations): {dataset_1_citations_per_200_words}\")\n",
        "print(f\"Total in-text citations in Dataset 2 (Peer-Reviewed Articles): {dataset_2_citations}\")\n",
        "print(f\"In-text citations per 200 words in Dataset 2 (Peer-Reviewed Articles): {dataset_2_citations_per_200_words}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Function to write the analysis summary to multiple CSV tables for diagram transformation\n",
        "def write_summary_to_csv_tables(output_file):\n",
        "    # Define the headers for each table\n",
        "    tables = [\n",
        "        {\n",
        "            \"filename\": \"word_count_analysis.csv\",\n",
        "            \"headers\": [\"Dataset Type\", \"Total Number of Words\"],\n",
        "            \"data\": [\n",
        "                [\"Dataset 1 (Dissertations)\", 173859],\n",
        "                [\"Dataset 2 (Peer-Reviewed Articles)\", 71779]\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"filename\": \"in_text_citations_analysis.csv\",\n",
        "            \"headers\": [\"Dataset Type\", \"Total In-Text Citations\", \"In-Text Citations per 200 Words\"],\n",
        "            \"data\": [\n",
        "                [\"Dataset 1 (Dissertations)\", 1088, 1.2514593649533865],\n",
        "                [\"Dataset 2 (Peer-Reviewed Articles)\", 324, 0.8696935940624623]\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"filename\": \"content_density_analysis.csv\",\n",
        "            \"headers\": [\"Dataset Type\", \"Total Number of Ideas\", \"Total Number of New Concepts\"],\n",
        "            \"data\": [\n",
        "                [\"Dataset 1 (Dissertations)\", 10265, 24744],\n",
        "                [\"Dataset 2 (Articles)\", 4450, 11575]\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"filename\": \"ideational_meta_function_analysis.csv\",\n",
        "            \"headers\": [\"Dataset Type\", \"Processes (Material)\", \"Processes (Mental)\", \"Processes (Relational)\", \"Participants Count\", \"Circumstances Count\"],\n",
        "            \"data\": [\n",
        "                [\"Dataset 1 (Dissertations)\", 475, 147, 247, 72931, 23100],\n",
        "                [\"Dataset 2 (Articles)\", 269, 122, 111, 32941, 8994]\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"filename\": \"interpersonal_meta_function_analysis.csv\",\n",
        "            \"headers\": [\"Dataset Type\", \"Declarative Clauses\", \"Interrogative Clauses\", \"Imperative Clauses\", \"Modality Count\", \"Pronouns & Terms Count\"],\n",
        "            \"data\": [\n",
        "                [\"Dataset 1 (Dissertations)\", 4801, 564, 70, 1432, 4310],\n",
        "                [\"Dataset 2 (Articles)\", 1770, 239, 92, 512, 1884]\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"filename\": \"textual_meta_function_analysis.csv\",\n",
        "            \"headers\": [\"Dataset Type\", \"Theme & Rheme Count\", \"Conjunction Count\", \"Reference Words Count\", \"Substitution Count\", \"Ellipses Count\", \"Lexical Chains Count\"],\n",
        "            \"data\": [\n",
        "                [\"Dataset 1 (Dissertations)\", 10265, 7516, 4310, 385, 23919, 51685],\n",
        "                [\"Dataset 2 (Articles)\", 4450, 3318, 1884, 256, 0, 22184]\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Write each table to a CSV file\n",
        "    for table in tables:\n",
        "        with open(f\"/content/drive/MyDrive/{table['filename']}\", mode='w', newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow(table[\"headers\"])\n",
        "            for row in table[\"data\"]:\n",
        "                writer.writerow(row)\n",
        "\n",
        "# Write the summary to CSV tables\n",
        "write_summary_to_csv_tables(output_file=\"summary_analysis_tables\")\n",
        "\n",
        "print(\"Summary analysis tables saved to Google Drive\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ky15OjqlbaVE",
        "outputId": "8c61c0c0-2429-4aef-df32-8fcfecf2c9e7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary analysis tables saved to Google Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-_-qUyBFPk-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6YK1_tMFfoX",
        "outputId": "e7d8a944-c2bc-4541-d9fd-12ce5250f820"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence types in Dataset 1 (Dissertations): {'type_1': 0, 'type_2': 0}\n",
            "Sentence types in Dataset 2 (Peer-Reviewed Articles): {'type_1': 0, 'type_2': 0}\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from pdfminer.high_level import extract_text\n",
        "\n",
        "# Function to count types of sentence structures in a PDF\n",
        "def count_sentence_types(file_path):\n",
        "    # Extract text using pdfminer\n",
        "    text = extract_text(file_path)\n",
        "\n",
        "    # Patterns for different types of sentence structures\n",
        "    sentence_patterns = {\n",
        "        \"type_1\": r\"\\b(clause\\s+(and|or|nor|so|for|but|yet)\\s+clause)\\b\",  # Type 1: Clause + conjunction (and, or, nor, so, for, but, yet)\n",
        "        \"type_2\": r\"\\b(clause,\\s+(when|since|while|if|whenever|because|wherever|that|which)\\s+clause)\\b\"  # Type 2: Clause + subordinating conjunction\n",
        "    }\n",
        "\n",
        "    # Count occurrences of each sentence type\n",
        "    sentence_counts = {key: len(re.findall(pattern, text)) for key, pattern in sentence_patterns.items()}\n",
        "\n",
        "    return sentence_counts\n",
        "\n",
        "# Define paths for the datasets manually mounted in Google Colab\n",
        "dataset_1_path = '/content/drive/MyDrive/dataset_1.pdf'  # Path to Dataset 1 (Dissertations)\n",
        "dataset_2_path = '/content/drive/MyDrive/dataset_2.pdf'  # Path to Dataset 2 (Peer-Reviewed Articles)\n",
        "\n",
        "# Count the sentence types for Dataset 1 and Dataset 2\n",
        "dataset_1_sentence_types = count_sentence_types(dataset_1_path)\n",
        "dataset_2_sentence_types = count_sentence_types(dataset_2_path)\n",
        "\n",
        "print(f\"Sentence types in Dataset 1 (Dissertations): {dataset_1_sentence_types}\")\n",
        "print(f\"Sentence types in Dataset 2 (Peer-Reviewed Articles): {dataset_2_sentence_types}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "v4yR3YEGGTsV",
        "outputId": "33b06061-9770-475f-ccf2-200b724479c7"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "[E088] Text of length 1343559 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-a8a68d3c11f4>\u001b[0m in \u001b[0;36m<cell line: 43>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Analyze sentence complexity for Dataset 1 and Dataset 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mdataset_1_complexity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyze_sentence_complexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_1_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0mdataset_2_complexity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyze_sentence_complexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_2_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-a8a68d3c11f4>\u001b[0m in \u001b[0;36manalyze_sentence_complexity\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Process the text using spaCy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtotal_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \u001b[0mDOCS\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;31m#call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \"\"\"\n\u001b[0;32m-> 1037\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcomponent_cfg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m             \u001b[0mcomponent_cfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m_ensure_doc\u001b[0;34m(self, doc_like)\u001b[0m\n\u001b[1;32m   1126\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdoc_like\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36mmake_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \"\"\"\n\u001b[1;32m   1116\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1118\u001b[0m                 \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: [E088] Text of length 1343559 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`."
          ]
        }
      ],
      "source": [
        "import re\n",
        "from pdfminer.high_level import extract_text\n",
        "import spacy\n",
        "\n",
        "# Load spaCy's English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to determine sentence complexity using average sentence length and number of clauses\n",
        "def analyze_sentence_complexity(file_path):\n",
        "    # Extract text using pdfminer\n",
        "    text = extract_text(file_path)\n",
        "\n",
        "    # Process the text using spaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    total_sentences = 0\n",
        "    total_words = 0\n",
        "    total_clauses = 0\n",
        "\n",
        "    for sent in doc.sents:\n",
        "        total_sentences += 1\n",
        "        words = [token.text for token in sent if token.is_alpha]\n",
        "        total_words += len(words)\n",
        "\n",
        "        # Count the number of clauses based on the number of verbs in the sentence\n",
        "        clauses = [token for token in sent if token.pos_ == \"VERB\"]\n",
        "        total_clauses += len(clauses)\n",
        "\n",
        "    # Calculate average sentence length and average clauses per sentence\n",
        "    avg_sentence_length = total_words / total_sentences if total_sentences > 0 else 0\n",
        "    avg_clauses_per_sentence = total_clauses / total_sentences if total_sentences > 0 else 0\n",
        "\n",
        "    return {\n",
        "        \"average_sentence_length\": avg_sentence_length,\n",
        "        \"average_clauses_per_sentence\": avg_clauses_per_sentence\n",
        "    }\n",
        "\n",
        "# Define paths for the datasets manually mounted in Google Colab\n",
        "dataset_1_path = '/content/drive/MyDrive/dataset_1.pdf'  # Path to Dataset 1 (Dissertations)\n",
        "dataset_2_path = '/content/drive/MyDrive/dataset_2.pdf'  # Path to Dataset 2 (Peer-Reviewed Articles)\n",
        "\n",
        "# Analyze sentence complexity for Dataset 1 and Dataset 2\n",
        "dataset_1_complexity = analyze_sentence_complexity(dataset_1_path)\n",
        "dataset_2_complexity = analyze_sentence_complexity(dataset_2_path)\n",
        "\n",
        "print(f\"Sentence complexity in Dataset 1 (Dissertations): {dataset_1_complexity}\")\n",
        "print(f\"Sentence complexity in Dataset 2 (Peer-Reviewed Articles): {dataset_2_complexity}\")\n",
        "\n",
        "# Determine which dataset has more complex sentences\n",
        "if dataset_1_complexity[\"average_clauses_per_sentence\"] > dataset_2_complexity[\"average_clauses_per_sentence\"]:\n",
        "    print(\"Dataset 1 (Dissertations) has more complex sentences.\")\n",
        "elif dataset_1_complexity[\"average_clauses_per_sentence\"] < dataset_2_complexity[\"average_clauses_per_sentence\"]:\n",
        "    print(\"Dataset 2 (Peer-Reviewed Articles) has more complex sentences.\")\n",
        "else:\n",
        "    print(\"Both datasets have similar sentence complexity.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArgpB_5mG02i",
        "outputId": "370191df-ecf5-420a-8cab-5719c7fa798c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence complexity in Dataset 1 (Dissertations): {'average_sentence_length': 15.555598907530237, 'average_clauses_per_sentence': 1.583008193523215}\n",
            "Sentence complexity in Dataset 2 (Peer-Reviewed Articles): {'average_sentence_length': 15.280026990553306, 'average_clauses_per_sentence': 1.6360773729194782}\n",
            "Dataset 2 (Peer-Reviewed Articles) has more complex sentences.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from pdfminer.high_level import extract_text\n",
        "import spacy\n",
        "\n",
        "# Load spaCy's English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Increase spaCy's max length limit to handle larger texts\n",
        "nlp.max_length = 1500000\n",
        "\n",
        "# Function to determine sentence complexity using average sentence length and number of clauses\n",
        "def analyze_sentence_complexity(file_path):\n",
        "    # Extract text using pdfminer\n",
        "    text = extract_text(file_path)\n",
        "\n",
        "    # Process the text using spaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    total_sentences = 0\n",
        "    total_words = 0\n",
        "    total_clauses = 0\n",
        "\n",
        "    for sent in doc.sents:\n",
        "        total_sentences += 1\n",
        "        words = [token.text for token in sent if token.is_alpha]\n",
        "        total_words += len(words)\n",
        "\n",
        "        # Count the number of clauses based on the number of verbs in the sentence\n",
        "        clauses = [token for token in sent if token.pos_ == \"VERB\"]\n",
        "        total_clauses += len(clauses)\n",
        "\n",
        "    # Calculate average sentence length and average clauses per sentence\n",
        "    avg_sentence_length = total_words / total_sentences if total_sentences > 0 else 0\n",
        "    avg_clauses_per_sentence = total_clauses / total_sentences if total_sentences > 0 else 0\n",
        "\n",
        "    return {\n",
        "        \"average_sentence_length\": avg_sentence_length,\n",
        "        \"average_clauses_per_sentence\": avg_clauses_per_sentence\n",
        "    }\n",
        "\n",
        "# Define paths for the datasets manually mounted in Google Colab\n",
        "dataset_1_path = '/content/drive/MyDrive/dataset_1.pdf'  # Path to Dataset 1 (Dissertations)\n",
        "dataset_2_path = '/content/drive/MyDrive/dataset_2.pdf'  # Path to Dataset 2 (Peer-Reviewed Articles)\n",
        "\n",
        "# Analyze sentence complexity for Dataset 1 and Dataset 2\n",
        "dataset_1_complexity = analyze_sentence_complexity(dataset_1_path)\n",
        "dataset_2_complexity = analyze_sentence_complexity(dataset_2_path)\n",
        "\n",
        "print(f\"Sentence complexity in Dataset 1 (Dissertations): {dataset_1_complexity}\")\n",
        "print(f\"Sentence complexity in Dataset 2 (Peer-Reviewed Articles): {dataset_2_complexity}\")\n",
        "\n",
        "# Determine which dataset has more complex sentences\n",
        "if dataset_1_complexity[\"average_clauses_per_sentence\"] > dataset_2_complexity[\"average_clauses_per_sentence\"]:\n",
        "    print(\"Dataset 1 (Dissertations) has more complex sentences.\")\n",
        "elif dataset_1_complexity[\"average_clauses_per_sentence\"] < dataset_2_complexity[\"average_clauses_per_sentence\"]:\n",
        "    print(\"Dataset 2 (Peer-Reviewed Articles) has more complex sentences.\")\n",
        "else:\n",
        "    print(\"Both datasets have similar sentence complexity.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "bso301BtJArT",
        "outputId": "41a70e56-1ace-47dd-daf5-e87a9a57ed39"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pdfminer'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-25905cfc50d4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpdfminer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh_level\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mextract_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load spaCy's English model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_core_web_sm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pdfminer'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from pdfminer.high_level import extract_text\n",
        "import spacy\n",
        "\n",
        "# Load spaCy's English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Increase spaCy's max length limit to handle larger texts\n",
        "nlp.max_length = 1500000\n",
        "\n",
        "# Function to analyze content density by counting ideas and new concepts\n",
        "def analyze_content_density(file_path):\n",
        "    # Extract text using pdfminer\n",
        "    text = extract_text(file_path)\n",
        "\n",
        "    # Process the text using spaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    total_ideas = 0\n",
        "    new_concepts = set()\n",
        "\n",
        "    # Iterate through sentences to identify ideas and concepts\n",
        "    for sent in doc.sents:\n",
        "        # Consider each sentence as a potential new idea\n",
        "        total_ideas += 1\n",
        "\n",
        "        # Identify nouns and noun phrases to count as new concepts\n",
        "        for chunk in sent.noun_chunks:\n",
        "            concept = chunk.text.lower()\n",
        "            new_concepts.add(concept)\n",
        "\n",
        "    total_new_concepts = len(new_concepts)\n",
        "\n",
        "    return {\n",
        "        \"total_ideas\": total_ideas,\n",
        "        \"total_new_concepts\": total_new_concepts\n",
        "    }\n",
        "\n",
        "# Define paths for the datasets manually mounted in Google Colab\n",
        "dataset_1_path = '/content/drive/MyDrive/dataset_1.pdf'  # Path to Dataset 1 (Dissertations)\n",
        "dataset_2_path = '/content/drive/MyDrive/dataset_2.pdf'  # Path to Dataset 2 (Peer-Reviewed Articles)\n",
        "\n",
        "# Analyze content density for Dataset 1 and Dataset 2\n",
        "dataset_1_density = analyze_content_density(dataset_1_path)\n",
        "dataset_2_density = analyze_content_density(dataset_2_path)\n",
        "\n",
        "print(f\"Content density in Dataset 1 (Dissertations): {dataset_1_density}\")\n",
        "print(f\"Content density in Dataset 2 (Peer-Reviewed Articles): {dataset_2_density}\")\n",
        "\n",
        "# Output results in the required format\n",
        "print(\"\\nSheet 5: Content Density Analysis\")\n",
        "print(\"Dataset Type\\tTotal Number of Ideas\\tTotal Number of New Concepts\")\n",
        "print(f\"Dataset 1 (Dissertations)\\t{dataset_1_density['total_ideas']}\\t{dataset_1_density['total_new_concepts']}\")\n",
        "print(f\"Dataset 2 (Articles)\\t{dataset_2_density['total_ideas']}\\t{dataset_2_density['total_new_concepts']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfminer.six\n",
        "from pdfminer.high_level import extract_text\n",
        "import spacy\n",
        "\n",
        "# Load spaCy's English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Increase spaCy's max length limit to handle larger texts\n",
        "nlp.max_length = 1500000\n",
        "\n",
        "# Function to analyze content density by counting ideas and new concepts\n",
        "def analyze_content_density(file_path):\n",
        "    # Extract text using pdfminer\n",
        "    text = extract_text(file_path)\n",
        "\n",
        "    # Split the text into manageable chunks to avoid excessive processing time\n",
        "    text_chunks = [text[i:i+100000] for i in range(0, len(text), 100000)]\n",
        "\n",
        "    total_ideas = 0\n",
        "    new_concepts = set()\n",
        "\n",
        "    # Process each chunk separately to improve performance\n",
        "    for chunk in text_chunks:\n",
        "        doc = nlp(chunk)\n",
        "\n",
        "        # Iterate through sentences to identify ideas and concepts\n",
        "        for sent in doc.sents:\n",
        "            # Consider each sentence as a potential new idea\n",
        "            total_ideas += 1\n",
        "\n",
        "            # Identify nouns and noun phrases to count as new concepts\n",
        "            for noun_chunk in sent.noun_chunks:\n",
        "                concept = noun_chunk.text.lower()\n",
        "                new_concepts.add(concept)\n",
        "\n",
        "    total_new_concepts = len(new_concepts)\n",
        "\n",
        "    return {\n",
        "        \"total_ideas\": total_ideas,\n",
        "        \"total_new_concepts\": total_new_concepts\n",
        "    }\n",
        "\n",
        "# Define paths for the datasets manually mounted in Google Colab\n",
        "dataset_1_path = '/content/drive/MyDrive/dataset_1.pdf'  # Path to Dataset 1 (Dissertations)\n",
        "dataset_2_path = '/content/drive/MyDrive/dataset_2.pdf'  # Path to Dataset 2 (Peer-Reviewed Articles)\n",
        "\n",
        "# Analyze content density for Dataset 1 and Dataset 2\n",
        "dataset_1_density = analyze_content_density(dataset_1_path)\n",
        "dataset_2_density = analyze_content_density(dataset_2_path)\n",
        "\n",
        "print(f\"Content density in Dataset 1 (Dissertations): {dataset_1_density}\")\n",
        "print(f\"Content density in Dataset 2 (Peer-Reviewed Articles): {dataset_2_density}\")\n",
        "\n",
        "# Output results in the required format\n",
        "print(\"\\nSheet 5: Content Density Analysis\")\n",
        "print(\"Dataset Type\\tTotal Number of Ideas\\tTotal Number of New Concepts\")\n",
        "print(f\"Dataset 1 (Dissertations)\\t{dataset_1_density['total_ideas']}\\t{dataset_1_density['total_new_concepts']}\")\n",
        "print(f\"Dataset 2 (Articles)\\t{dataset_2_density['total_ideas']}\\t{dataset_2_density['total_new_concepts']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9N3qSb0WYIm",
        "outputId": "348d32b3-7230-4166-8890-ee3a29562547"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.10/dist-packages (20240706)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.4.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Content density in Dataset 1 (Dissertations): {'total_ideas': 10265, 'total_new_concepts': 24744}\n",
            "Content density in Dataset 2 (Peer-Reviewed Articles): {'total_ideas': 4450, 'total_new_concepts': 11575}\n",
            "\n",
            "Sheet 5: Content Density Analysis\n",
            "Dataset Type\tTotal Number of Ideas\tTotal Number of New Concepts\n",
            "Dataset 1 (Dissertations)\t10265\t24744\n",
            "Dataset 2 (Articles)\t4450\t11575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pdfminer.high_level import extract_text\n",
        "import spacy\n",
        "\n",
        "# Load spaCy's English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Increase spaCy's max length limit to handle larger texts\n",
        "nlp.max_length = 1500000\n",
        "\n",
        "# Function to perform Ideational Meta-Function Analysis\n",
        "def analyze_ideational_meta_function(file_path):\n",
        "    # Extract text using pdfminer\n",
        "    text = extract_text(file_path)\n",
        "\n",
        "    # Split the text into manageable chunks to avoid excessive processing time\n",
        "    text_chunks = [text[i:i+100000] for i in range(0, len(text), 100000)]\n",
        "\n",
        "    material_processes = 0\n",
        "    mental_processes = 0\n",
        "    relational_processes = 0\n",
        "    participants_count = 0\n",
        "    circumstances_count = 0\n",
        "\n",
        "    # Process each chunk separately to improve performance\n",
        "    for chunk in text_chunks:\n",
        "        doc = nlp(chunk)\n",
        "\n",
        "        # Iterate through tokens to analyze ideational meta-function elements\n",
        "        for token in doc:\n",
        "            # Count processes based on verb type\n",
        "            if token.pos_ == \"VERB\":\n",
        "                if token.tag_ in [\"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]:  # General verb tags\n",
        "                    # Classify verb types as material, mental, or relational\n",
        "                    if token.lemma_ in [\"do\", \"make\", \"build\", \"create\", \"move\"]:\n",
        "                        material_processes += 1\n",
        "                    elif token.lemma_ in [\"think\", \"believe\", \"know\", \"feel\", \"understand\"]:\n",
        "                        mental_processes += 1\n",
        "                    elif token.lemma_ in [\"be\", \"seem\", \"become\", \"appear\"]:\n",
        "                        relational_processes += 1\n",
        "\n",
        "            # Count participants (nouns)\n",
        "            if token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\":\n",
        "                participants_count += 1\n",
        "\n",
        "            # Count circumstances (adverbs and prepositional phrases)\n",
        "            if token.pos_ == \"ADV\" or token.dep_ == \"prep\":\n",
        "                circumstances_count += 1\n",
        "\n",
        "    return {\n",
        "        \"material_processes\": material_processes,\n",
        "        \"mental_processes\": mental_processes,\n",
        "        \"relational_processes\": relational_processes,\n",
        "        \"participants_count\": participants_count,\n",
        "        \"circumstances_count\": circumstances_count\n",
        "    }\n",
        "\n",
        "# Define paths for the datasets manually mounted in Google Colab\n",
        "dataset_1_path = '/content/drive/MyDrive/dataset_1.pdf'  # Path to Dataset 1 (Dissertations)\n",
        "dataset_2_path = '/content/drive/MyDrive/dataset_2.pdf'  # Path to Dataset 2 (Peer-Reviewed Articles)\n",
        "\n",
        "# Analyze ideational meta-function for Dataset 1 and Dataset 2\n",
        "dataset_1_ideational = analyze_ideational_meta_function(dataset_1_path)\n",
        "dataset_2_ideational = analyze_ideational_meta_function(dataset_2_path)\n",
        "\n",
        "print(f\"Ideational Meta-Function Analysis for Dataset 1 (Dissertations): {dataset_1_ideational}\")\n",
        "print(f\"Ideational Meta-Function Analysis for Dataset 2 (Peer-Reviewed Articles): {dataset_2_ideational}\")\n",
        "\n",
        "# Output results in the required format\n",
        "print(\"\\nSheet 6: Ideational Meta-Function Analysis\")\n",
        "print(\"Dataset Type\\tProcesses (Material)\\tProcesses (Mental)\\tProcesses (Relational)\\tParticipants Count\\tCircumstances Count\")\n",
        "print(f\"Dataset 1 (Dissertations)\\t{dataset_1_ideational['material_processes']}\\t{dataset_1_ideational['mental_processes']}\\t{dataset_1_ideational['relational_processes']}\\t{dataset_1_ideational['participants_count']}\\t{dataset_1_ideational['circumstances_count']}\")\n",
        "print(f\"Dataset 2 (Articles)\\t{dataset_2_ideational['material_processes']}\\t{dataset_2_ideational['mental_processes']}\\t{dataset_2_ideational['relational_processes']}\\t{dataset_2_ideational['participants_count']}\\t{dataset_2_ideational['circumstances_count']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uS12bwG9YKQe",
        "outputId": "45562ae3-f6d5-4d27-a76f-c48e56116db0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ideational Meta-Function Analysis for Dataset 1 (Dissertations): {'material_processes': 475, 'mental_processes': 147, 'relational_processes': 247, 'participants_count': 72931, 'circumstances_count': 23100}\n",
            "Ideational Meta-Function Analysis for Dataset 2 (Peer-Reviewed Articles): {'material_processes': 269, 'mental_processes': 122, 'relational_processes': 111, 'participants_count': 32941, 'circumstances_count': 8994}\n",
            "\n",
            "Sheet 6: Ideational Meta-Function Analysis\n",
            "Dataset Type\tProcesses (Material)\tProcesses (Mental)\tProcesses (Relational)\tParticipants Count\tCircumstances Count\n",
            "Dataset 1 (Dissertations)\t475\t147\t247\t72931\t23100\n",
            "Dataset 2 (Articles)\t269\t122\t111\t32941\t8994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pdfminer.high_level import extract_text\n",
        "import spacy\n",
        "\n",
        "# Load spaCy's English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Increase spaCy's max length limit to handle larger texts\n",
        "nlp.max_length = 1500000\n",
        "\n",
        "# Function to perform Interpersonal Meta-Function Analysis\n",
        "def analyze_interpersonal_meta_function(file_path):\n",
        "    # Extract text using pdfminer\n",
        "    text = extract_text(file_path)\n",
        "\n",
        "    # Split the text into manageable chunks to avoid excessive processing time\n",
        "    text_chunks = [text[i:i+100000] for i in range(0, len(text), 100000)]\n",
        "\n",
        "    declarative_clauses = 0\n",
        "    interrogative_clauses = 0\n",
        "    imperative_clauses = 0\n",
        "    modality_count = 0\n",
        "    pronouns_terms_count = 0\n",
        "\n",
        "    # List of common modal verbs\n",
        "    modal_verbs = [\"can\", \"could\", \"will\", \"would\", \"shall\", \"should\", \"may\", \"might\", \"must\"]\n",
        "\n",
        "    # Process each chunk separately to improve performance\n",
        "    for chunk in text_chunks:\n",
        "        doc = nlp(chunk)\n",
        "\n",
        "        # Iterate through sentences to analyze clause types and modality\n",
        "        for sent in doc.sents:\n",
        "            # Determine clause type\n",
        "            if any(token.tag_ == \"VBP\" or token.tag_ == \"VBZ\" for token in sent):\n",
        "                declarative_clauses += 1\n",
        "            elif any(token.tag_ == \"MD\" and token.text.lower() in modal_verbs for token in sent):\n",
        "                interrogative_clauses += 1\n",
        "            elif any(token.dep_ == \"ROOT\" and token.pos_ == \"VERB\" and token.tag_ == \"VB\" for token in sent):\n",
        "                imperative_clauses += 1\n",
        "\n",
        "            # Count modality (e.g., can, must, might)\n",
        "            modality_count += sum(1 for token in sent if token.text.lower() in modal_verbs)\n",
        "\n",
        "            # Count pronouns and terms of address\n",
        "            pronouns_terms_count += sum(1 for token in sent if token.pos_ == \"PRON\")\n",
        "\n",
        "    return {\n",
        "        \"declarative_clauses\": declarative_clauses,\n",
        "        \"interrogative_clauses\": interrogative_clauses,\n",
        "        \"imperative_clauses\": imperative_clauses,\n",
        "        \"modality_count\": modality_count,\n",
        "        \"pronouns_terms_count\": pronouns_terms_count\n",
        "    }\n",
        "\n",
        "# Define paths for the datasets manually mounted in Google Colab\n",
        "dataset_1_path = '/content/drive/MyDrive/dataset_1.pdf'  # Path to Dataset 1 (Dissertations)\n",
        "dataset_2_path = '/content/drive/MyDrive/dataset_2.pdf'  # Path to Dataset 2 (Peer-Reviewed Articles)\n",
        "\n",
        "# Analyze interpersonal meta-function for Dataset 1 and Dataset 2\n",
        "dataset_1_interpersonal = analyze_interpersonal_meta_function(dataset_1_path)\n",
        "dataset_2_interpersonal = analyze_interpersonal_meta_function(dataset_2_path)\n",
        "\n",
        "print(f\"Interpersonal Meta-Function Analysis for Dataset 1 (Dissertations): {dataset_1_interpersonal}\")\n",
        "print(f\"Interpersonal Meta-Function Analysis for Dataset 2 (Peer-Reviewed Articles): {dataset_2_interpersonal}\")\n",
        "\n",
        "# Output results in the required format\n",
        "print(\"\\nSheet 7: Interpersonal Meta-Function Analysis\")\n",
        "print(\"Dataset Type\\tDeclarative Clauses\\tInterrogative Clauses\\tImperative Clauses\\tModality Count\\tPronouns & Terms Count\")\n",
        "print(f\"Dataset 1 (Dissertations)\\t{dataset_1_interpersonal['declarative_clauses']}\\t{dataset_1_interpersonal['interrogative_clauses']}\\t{dataset_1_interpersonal['imperative_clauses']}\\t{dataset_1_interpersonal['modality_count']}\\t{dataset_1_interpersonal['pronouns_terms_count']}\")\n",
        "print(f\"Dataset 2 (Articles)\\t{dataset_2_interpersonal['declarative_clauses']}\\t{dataset_2_interpersonal['interrogative_clauses']}\\t{dataset_2_interpersonal['imperative_clauses']}\\t{dataset_2_interpersonal['modality_count']}\\t{dataset_2_interpersonal['pronouns_terms_count']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4uZ3J0RY1P3",
        "outputId": "24a8fc3d-5e0b-4302-970b-508b5bef44a2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Interpersonal Meta-Function Analysis for Dataset 1 (Dissertations): {'declarative_clauses': 4801, 'interrogative_clauses': 564, 'imperative_clauses': 70, 'modality_count': 1432, 'pronouns_terms_count': 4310}\n",
            "Interpersonal Meta-Function Analysis for Dataset 2 (Peer-Reviewed Articles): {'declarative_clauses': 1770, 'interrogative_clauses': 239, 'imperative_clauses': 92, 'modality_count': 512, 'pronouns_terms_count': 1884}\n",
            "\n",
            "Sheet 7: Interpersonal Meta-Function Analysis\n",
            "Dataset Type\tDeclarative Clauses\tInterrogative Clauses\tImperative Clauses\tModality Count\tPronouns & Terms Count\n",
            "Dataset 1 (Dissertations)\t4801\t564\t70\t1432\t4310\n",
            "Dataset 2 (Articles)\t1770\t239\t92\t512\t1884\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pdfminer.high_level import extract_text\n",
        "import spacy\n",
        "\n",
        "# Load spaCy's English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Increase spaCy's max length limit to handle larger texts\n",
        "nlp.max_length = 1500000\n",
        "\n",
        "# Function to perform Textual Meta-Function Analysis\n",
        "def analyze_textual_meta_function(file_path):\n",
        "    # Extract text using pdfminer\n",
        "    text = extract_text(file_path)\n",
        "\n",
        "    # Split the text into manageable chunks to avoid excessive processing time\n",
        "    text_chunks = [text[i:i+100000] for i in range(0, len(text), 100000)]\n",
        "\n",
        "    theme_rheme_count = 0\n",
        "    conjunction_count = 0\n",
        "    reference_words_count = 0\n",
        "    substitution_count = 0\n",
        "    ellipses_count = 0\n",
        "    lexical_chains_count = 0\n",
        "\n",
        "    # List of common conjunctions\n",
        "    conjunctions = [\"and\", \"or\", \"but\", \"so\", \"because\", \"although\", \"if\", \"when\", \"while\"]\n",
        "\n",
        "    # Process each chunk separately to improve performance\n",
        "    for chunk in text_chunks:\n",
        "        doc = nlp(chunk)\n",
        "\n",
        "        # Iterate through sentences to analyze textual meta-function elements\n",
        "        for sent in doc.sents:\n",
        "            # Count theme and rheme (simplified as counting sentences)\n",
        "            theme_rheme_count += 1\n",
        "\n",
        "            # Count conjunctions\n",
        "            conjunction_count += sum(1 for token in sent if token.text.lower() in conjunctions)\n",
        "\n",
        "            # Count reference words (e.g., pronouns)\n",
        "            reference_words_count += sum(1 for token in sent if token.pos_ == \"PRON\")\n",
        "\n",
        "            # Count substitutions (e.g., \"one\", \"do so\")\n",
        "            substitution_count += sum(1 for token in sent if token.lemma_ in [\"one\", \"do\", \"so\"])\n",
        "\n",
        "            # Count ellipses (simplified as counting \"...\")\n",
        "            ellipses_count += sent.text.count(\"...\")\n",
        "\n",
        "        # Lexical chains (simplified as counting noun phrases)\n",
        "        lexical_chains_count += sum(1 for chunk in doc.noun_chunks)\n",
        "\n",
        "    return {\n",
        "        \"theme_rheme_count\": theme_rheme_count,\n",
        "        \"conjunction_count\": conjunction_count,\n",
        "        \"reference_words_count\": reference_words_count,\n",
        "        \"substitution_count\": substitution_count,\n",
        "        \"ellipses_count\": ellipses_count,\n",
        "        \"lexical_chains_count\": lexical_chains_count\n",
        "    }\n",
        "\n",
        "# Define paths for the datasets manually mounted in Google Colab\n",
        "dataset_1_path = '/content/drive/MyDrive/dataset_1.pdf'  # Path to Dataset 1 (Dissertations)\n",
        "dataset_2_path = '/content/drive/MyDrive/dataset_2.pdf'  # Path to Dataset 2 (Peer-Reviewed Articles)\n",
        "\n",
        "# Analyze textual meta-function for Dataset 1 and Dataset 2\n",
        "dataset_1_textual = analyze_textual_meta_function(dataset_1_path)\n",
        "dataset_2_textual = analyze_textual_meta_function(dataset_2_path)\n",
        "\n",
        "print(f\"Textual Meta-Function Analysis for Dataset 1 (Dissertations): {dataset_1_textual}\")\n",
        "print(f\"Textual Meta-Function Analysis for Dataset 2 (Peer-Reviewed Articles): {dataset_2_textual}\")\n",
        "\n",
        "# Output results in the required format\n",
        "print(\"\\nSheet 8: Textual Meta-Function Analysis\")\n",
        "print(\"Dataset Type\\tTheme & Rheme Count\\tConjunction Count\\tReference Words Count\\tSubstitution Count\\tEllipses Count\\tLexical Chains Count\")\n",
        "print(f\"Dataset 1 (Dissertations)\\t{dataset_1_textual['theme_rheme_count']}\\t{dataset_1_textual['conjunction_count']}\\t{dataset_1_textual['reference_words_count']}\\t{dataset_1_textual['substitution_count']}\\t{dataset_1_textual['ellipses_count']}\\t{dataset_1_textual['lexical_chains_count']}\")\n",
        "print(f\"Dataset 2 (Articles)\\t{dataset_2_textual['theme_rheme_count']}\\t{dataset_2_textual['conjunction_count']}\\t{dataset_2_textual['reference_words_count']}\\t{dataset_2_textual['substitution_count']}\\t{dataset_2_textual['ellipses_count']}\\t{dataset_2_textual['lexical_chains_count']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNWcyPO7ZkA2",
        "outputId": "7984e8eb-fb8b-4d61-97b9-35b3f0488ceb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Textual Meta-Function Analysis for Dataset 1 (Dissertations): {'theme_rheme_count': 10265, 'conjunction_count': 7516, 'reference_words_count': 4310, 'substitution_count': 385, 'ellipses_count': 23919, 'lexical_chains_count': 51685}\n",
            "Textual Meta-Function Analysis for Dataset 2 (Peer-Reviewed Articles): {'theme_rheme_count': 4450, 'conjunction_count': 3318, 'reference_words_count': 1884, 'substitution_count': 256, 'ellipses_count': 0, 'lexical_chains_count': 22184}\n",
            "\n",
            "Sheet 8: Textual Meta-Function Analysis\n",
            "Dataset Type\tTheme & Rheme Count\tConjunction Count\tReference Words Count\tSubstitution Count\tEllipses Count\tLexical Chains Count\n",
            "Dataset 1 (Dissertations)\t10265\t7516\t4310\t385\t23919\t51685\n",
            "Dataset 2 (Articles)\t4450\t3318\t1884\t256\t0\t22184\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Function to write the analysis summary to a CSV file\n",
        "def write_summary_to_csv(summary_data, output_file):\n",
        "    # Define the headers for the CSV file\n",
        "    headers = [\n",
        "        \"Dataset Type\", \"Total Number of Ideas\", \"Total Number of New Concepts\",\n",
        "        \"Processes (Material)\", \"Processes (Mental)\", \"Processes (Relational)\",\n",
        "        \"Participants Count\", \"Circumstances Count\", \"Declarative Clauses\",\n",
        "        \"Interrogative Clauses\", \"Imperative Clauses\", \"Modality Count\",\n",
        "        \"Pronouns & Terms Count\", \"Theme & Rheme Count\", \"Conjunction Count\",\n",
        "        \"Reference Words Count\", \"Substitution Count\", \"Ellipses Count\", \"Lexical Chains Count\"\n",
        "    ]\n",
        "\n",
        "    # Write the data to CSV\n",
        "    with open(output_file, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(headers)\n",
        "        for data in summary_data:\n",
        "            writer.writerow(data)\n",
        "\n",
        "# Prepare the summary data for both datasets\n",
        "summary_data = [\n",
        "    [\n",
        "        \"Dataset 1 (Dissertations)\",\n",
        "        dataset_1_density['total_ideas'], dataset_1_density['total_new_concepts'],\n",
        "        dataset_1_ideational['material_processes'], dataset_1_ideational['mental_processes'], dataset_1_ideational['relational_processes'],\n",
        "        dataset_1_ideational['participants_count'], dataset_1_ideational['circumstances_count'],\n",
        "        dataset_1_interpersonal['declarative_clauses'], dataset_1_interpersonal['interrogative_clauses'], dataset_1_interpersonal['imperative_clauses'],\n",
        "        dataset_1_interpersonal['modality_count'], dataset_1_interpersonal['pronouns_terms_count'],\n",
        "        dataset_1_textual['theme_rheme_count'], dataset_1_textual['conjunction_count'], dataset_1_textual['reference_words_count'],\n",
        "        dataset_1_textual['substitution_count'], dataset_1_textual['ellipses_count'], dataset_1_textual['lexical_chains_count']\n",
        "    ],\n",
        "    [\n",
        "        \"Dataset 2 (Articles)\",\n",
        "        dataset_2_density['total_ideas'], dataset_2_density['total_new_concepts'],\n",
        "        dataset_2_ideational['material_processes'], dataset_2_ideational['mental_processes'], dataset_2_ideational['relational_processes'],\n",
        "        dataset_2_ideational['participants_count'], dataset_2_ideational['circumstances_count'],\n",
        "        dataset_2_interpersonal['declarative_clauses'], dataset_2_interpersonal['interrogative_clauses'], dataset_2_interpersonal['imperative_clauses'],\n",
        "        dataset_2_interpersonal['modality_count'], dataset_2_interpersonal['pronouns_terms_count'],\n",
        "        dataset_2_textual['theme_rheme_count'], dataset_2_textual['conjunction_count'], dataset_2_textual['reference_words_count'],\n",
        "        dataset_2_textual['substitution_count'], dataset_2_textual['ellipses_count'], dataset_2_textual['lexical_chains_count']\n",
        "    ]\n",
        "]\n",
        "\n",
        "# Write the summary to a CSV file\n",
        "output_file = '/content/drive/MyDrive/summary_analysis.csv'\n",
        "write_summary_to_csv(summary_data, output_file)\n",
        "\n",
        "print(f\"Summary analysis saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhgOUiviaUEG",
        "outputId": "596289b8-56a2-4e2c-edb4-867b35584723"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary analysis saved to /content/drive/MyDrive/summary_analysis.csv\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1-VkN7tmcT3iDwRn0MAgmI1mxzPQZin4g",
      "authorship_tag": "ABX9TyOAnfs+3qzk75+hecgOYPaP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}